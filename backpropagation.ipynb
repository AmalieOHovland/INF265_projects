{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Backpropagation\n",
    "\n",
    "See project 1 text for a more detailed description of this task. Also see associated PDF report.\n",
    "\n",
    "Import relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tests_backpropagation import main_test\n",
    "\n",
    "# set seed and default data type\n",
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ``MyNet``\n",
    "\n",
    "##### NOTE: Amalie has edited this to fit what is actually in MyNet\n",
    "\n",
    "Read carefully how ``MyNet`` is implemented in the cell below. In particular:  \n",
    "- ``n_l`` is a list of integers, representing the number units in each layer (including input layer).\n",
    "-  ``MyNet([2, 3, 2]) = MiniNet()`` where ``MiniNet`` is the neural network defined in the fourth tutorial, in which notations are also clarified.     \n",
    "- ``model.L`` is the number of layers, ``L`` (countes as the hidden layers and the output layer)\n",
    "- ``model.f[l]`` is the activation function of layer ``l``, $f^{[l]}$ (here ``torch.tanh``)   \n",
    "- ``model.df[l]`` is the derivative of the activation function, $f'^{[l]}$   \n",
    "- ``model.a[l]``  is the tensor $A^{[l]}$, (shape: ``(1, n(l))``)   \n",
    "- ``model.z[l]``  is the tensor $Z^{[l]}$, (shape: ``(1, n(l))``)  \n",
    "- Weights $W^{[l]}$ (shape: ``(n(l+1), n(l))``) and biases $\\mathbf{b}^{[l]}$ (shape: ``(n(l+1))``) can be accessed as follows:\n",
    "```\n",
    "weights = model.fc[str(l)].weight.data\n",
    "bias = model.fc[str(l)].bias.data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - multi layer perception\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, n_l = [2, 3, 2]):\n",
    "        super().__init__() \n",
    "        \n",
    "        # number of layers in our network (following Andrew's notations)\n",
    "        self.L = len(n_l) - 1\n",
    "        self.n_l = n_l\n",
    "        \n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a = f(z))\n",
    "        self.z = {i : None for i in range(1, self.L+1)}\n",
    "        self.a = {i : None for i in range(self.L+1)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algorithm\n",
    "        self.dL_dw = {i : None for i in range(1, self.L+1)}\n",
    "        self.dL_db = {i : None for i in range(1, self.L+1)}\n",
    "\n",
    "        # Our activation functions in layers 1 through L (tanh)\n",
    "        self.f = {i : lambda x : torch.tanh(x) for i in range(1, self.L+1)}\n",
    "\n",
    "        # Derivatives of our activation functions in layers 1 through L\n",
    "        self.df = {i : lambda x : (1/(torch.cosh(x)**2)) for i in range(1, self.L+1)}\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # We have to use nn.ModuleDict and to use strings as keys here to \n",
    "        # respect pytorch requirements (otherwise, the model does not learn)\n",
    "        self.fc = nn.ModuleDict({str(i): None for i in range(1, self.L+1)})\n",
    "        for i in range(1, self.L+1):\n",
    "            self.fc[str(i)] = nn.Linear(in_features=n_l[i-1], out_features=n_l[i])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # Hidden layers until output layer\n",
    "        for i in range(1, self.L+1):\n",
    "\n",
    "            # fully connected layer\n",
    "            self.z[i] = self.fc[str(i)](self.a[i-1])\n",
    "            # activation\n",
    "            self.a[i] = self.f[i](self.z[i])\n",
    "\n",
    "        # return output\n",
    "        return self.a[self.L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Write a function ``backpropagation(model, y_true, y_pred)`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[l][i,j]`` for $l \\in [1 .. L]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[l][j]`` for $l \\in [1 .. L]$ \n",
    "\n",
    "assuming ``model`` is an instance of the ``MyNet`` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mean squared error (MSE) for the loss function (notation for one training example):\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\theta)=||\\mathrm{\\mathbf{y}}-\\mathrm{\\mathbf{\\hat{y}}}||^2_2\n",
    "\\end{equation}\n",
    "where $\\theta$ is all the parameters to be optimized, $\\mathrm{\\mathbf{y}}$ is expected/true labels, and $\\mathrm{\\mathbf{\\hat{y}}}$ are the predicted labels. The loss function describes how far the outputs are from the expected results.\n",
    "\n",
    "The weights are part of $\\theta$, and the derivative for the weights is given in Equation (4) in the project text:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w^{[l]}_{i,j}}=\\delta^{[l]}_i\\times a^{[l-1]}_j,\n",
    "\\end{equation}\n",
    "where $\\delta_i^{[l]}$ is the local gradient:\n",
    "\\begin{equation}\n",
    "    \\delta^{[l]}_i=\\frac{\\partial \\mathcal{L}}{\\partial z^{[l]}_i}=\\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}_i}\\times \\frac{\\partial a^{[l]}_i}{\\partial z^{[l]}_i}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Equation (5) shows the local derivative in the case for the output layer where $\\mathrm{\\mathbf{a}}^{[L]}=\\mathrm{\\mathbf{\\hat{y}}}$:\n",
    "    \n",
    "\\begin{equation}\n",
    "    \\delta^{[l]}_i=\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_i}\\times \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_i}=-2(y_i-\\hat{y}_i)\\times f'^{[L]}_i(z^{[L]}_i)=2(\\hat{y}_i-y_i)\\times f'^{[L]}_i(z^{[L]}_i)\n",
    "\\end{equation}\n",
    "\n",
    "And Equation (6) whos the local gradient for the general case:\n",
    "\\begin{equation}\n",
    "    \\delta^{[l]}_i=\\frac{\\partial \\mathcal{L}}{\\partial a^{[l]}_i}\\times \\frac{\\partial a^{[l]}_i}{\\partial z^{[l]}_i}=\\left(\\sum_{k=1}^{n^{[l+1]}} \\frac{\\partial \\mathcal{L}}{\\partial a^{[l+1]}_k}\\frac{\\partial a^{[l+1]}_k}{\\partial a^{[l]}_i}\\right)\\times \\frac{\\partial a^{[l]}_i}{\\partial z^{[l]}_i}=\\left(\\sum_{k=1}^{n^{[l+1]}} \\delta^{[l+1]}_k w^{[l+1]}_{k,i}\\right)\\times f'^{[l]}_i(z^{[l]}_i)\n",
    "\\end{equation}\n",
    "\n",
    "and we have that for the biases $\\mathrm{\\mathbf{b}}^{[l]}$ (also belonging to $\\theta$):\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}_j}=\\delta^{[l]}_j\\times 1\n",
    "\\end{equation}\n",
    "\n",
    "In the code, the derivatives are noted with underscore instead of fraction. An additional underscore is added if a specific layer is referenced. For example, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{[L]}}$ is written as ``dL_da_L``. In addition, $f'^{[L]}(\\mathbf{z}^{[L]})$ is denoted ``df_z_L``. Comments on dimensions are written as if $m=1$ (mini-batch) but the code is vectorized and works for training with multiple training examples ($m>1$). ``torch.einsum`` is used to ensure correct dimensions of the tensors when multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(model, y_true, y_pred):\n",
    "    ''' \n",
    "    backpropagation using existing a, z, weight and bias from forward prop \n",
    "    calculates dL/dw and dL/db for all layers in [1,...,L] and updates \n",
    "    MyNet model\n",
    "    '''\n",
    "\n",
    "    ##### Last layer L is first in backprop\n",
    "    \n",
    "    dL_da_L = 2*torch.sub(y_pred, y_true) # n^[L] x 1, input for first backprop layer (dL/da, eq. 5)\n",
    "\n",
    "    df_z_L = model.df[model.L](model.z[model.L]) # n^[L] x 1, da/dz=f'(z) for first backprop layer\n",
    "    \n",
    "    # local gradient in output layer - eq. 5 in project text\n",
    "    delta_L = torch.einsum('ij, ij -> ij', dL_da_L, df_z_L) # n^[L] x 1\n",
    "\n",
    "    # eq. 4 in project text\n",
    "    dL_dw_L = torch.einsum('ij, ik -> jk', delta_L, model.a[model.L-1]) # n^[L] x n^[L-1]\n",
    "    \n",
    "    # update in model\n",
    "    model.dL_dw[model.L] = dL_dw_L\n",
    "    model.dL_db[model.L] = torch.squeeze(delta_L) # squeeze to drop extra dimension\n",
    "\n",
    "    # save delta for backprop (eq. 6)\n",
    "    delta_ps = delta_L\n",
    "    \n",
    "    #### loop through hidden layers, [1,...,L-1], from last to first\n",
    "    for l in range(model.L-1, 0, -1): \n",
    "        \n",
    "        # f'(z) for current layer - l\n",
    "        df_z = model.df[l](model.z[l]) # n^[l] x 1\n",
    "        \n",
    "        # weights from previous backprop step (eq. 6)\n",
    "        weights_ps = model.fc[str(l+1)].weight.data # n^[l+1] x n^[l]\n",
    "\n",
    "        # eq. 6 in project text\n",
    "        delta_current = torch.einsum('ij,jk,ik->ik', delta_ps, weights_ps, df_z)\n",
    "\n",
    "        dL_dw = torch.einsum('ij,ik->jk', delta_current, model.a[l-1])\n",
    "        \n",
    "        # update in model\n",
    "        model.dL_dw[l] = dL_dw\n",
    "        model.dL_db[l] = torch.squeeze(delta_current)\n",
    "        \n",
    "        # save delta for further backprop (eq. 6)\n",
    "        delta_ps = delta_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cells below, and check the output\n",
    "\n",
    "- In the 1st cell, we use a toy dataset and the same architecture as the MiniNet class of the fourth tutorial. \n",
    "- In the 2nd cell, we use a few samples of the MNIST dataset with a consistent model architecture (``24x24`` black and white cropped images as input and ``10`` output classes). \n",
    "\n",
    "You can set ``verbose`` to ``True`` if you want more details about your computations versus what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================  Check gradients ======================== \n",
      "\n",
      " ================= Epoch 1 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 6.7814e-08,  7.2355e-08],\n",
      "        [-1.0720e-03, -1.1438e-03],\n",
      "        [-2.2284e-03, -2.3776e-03]], grad_fn=<ViewBackward>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 6.7814e-08,  7.2355e-08],\n",
      "        [-1.0720e-03, -1.1438e-03],\n",
      "        [-2.2284e-03, -2.3776e-03]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 8.0769e-09, -1.2768e-04, -2.6541e-04], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 8.0769e-09, -1.2768e-04, -2.6541e-04])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 2 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 3.5432e-08,  3.7805e-08],\n",
      "        [-4.1697e-04, -4.4489e-04],\n",
      "        [-1.0000e-03, -1.0670e-03]], grad_fn=<ViewBackward>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 3.5432e-08,  3.7805e-08],\n",
      "        [-4.1697e-04, -4.4489e-04],\n",
      "        [-1.0000e-03, -1.0670e-03]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 4.2201e-09, -4.9663e-05, -1.1910e-04], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 4.2201e-09, -4.9663e-05, -1.1910e-04])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 3 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 2.5834e-08,  2.7564e-08],\n",
      "        [-2.4624e-04, -2.6274e-04],\n",
      "        [-6.4282e-04, -6.8587e-04]], grad_fn=<ViewBackward>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 2.5834e-08,  2.7564e-08],\n",
      "        [-2.4624e-04, -2.6274e-04],\n",
      "        [-6.4282e-04, -6.8587e-04]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 3.0769e-09, -2.9329e-05, -7.6563e-05], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 3.0769e-09, -2.9329e-05, -7.6563e-05])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 4 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 2.1521e-08,  2.2962e-08],\n",
      "        [-1.7231e-04, -1.8385e-04],\n",
      "        [-4.7603e-04, -5.0791e-04]], grad_fn=<ViewBackward>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 2.1521e-08,  2.2962e-08],\n",
      "        [-1.7231e-04, -1.8385e-04],\n",
      "        [-4.7603e-04, -5.0791e-04]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 2.5632e-09, -2.0523e-05, -5.6697e-05], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 2.5632e-09, -2.0523e-05, -5.6697e-05])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ================= Epoch 5 ================= \n",
      "\n",
      " ------------ fc['1'].weight.grad ------------ \n",
      "  Our computation:\n",
      " tensor([[ 1.9401e-08,  2.0700e-08],\n",
      "        [-1.3261e-04, -1.4149e-04],\n",
      "        [-3.8135e-04, -4.0689e-04]], grad_fn=<ViewBackward>)\n",
      "  Autograd's computation:\n",
      " tensor([[ 1.9401e-08,  2.0700e-08],\n",
      "        [-1.3261e-04, -1.4149e-04],\n",
      "        [-3.8135e-04, -4.0689e-04]])\n",
      "\n",
      " ------------- fc['1'].bias.grad ------------- \n",
      "  Our computation:\n",
      " tensor([ 2.3107e-09, -1.5794e-05, -4.5420e-05], grad_fn=<SqueezeBackward0>)\n",
      "  Autograd's computation:\n",
      " tensor([ 2.3107e-09, -1.5794e-05, -4.5420e-05])\n",
      "\n",
      " ------------- relative error ------------ \n",
      "(fc[1].weight.grad, model.dL_dw[1]):   0.0000\n",
      "(fc[1].bias.grad, model.dL_db[1]):   0.0000\n",
      "(fc[2].weight.grad, model.dL_dw[2]):   0.0000\n",
      "(fc[2].bias.grad, model.dL_db[2]):   0.0000\n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ==============  Check that weights have been updated ============= \n",
      "tensor([[-0.3581, -0.7438],\n",
      "        [ 0.5436, -0.0012],\n",
      "        [ 0.1644,  0.2690]])\n",
      "tensor([0.0704, 0.3112, 0.7295])\n",
      "Weights have been updated. :)\n",
      "\n",
      " ===================  Check computational graph =================== \n",
      "All parameters seem correctly attached to the computational graph! :) \n"
     ]
    }
   ],
   "source": [
    "model = MyNet([2, 3, 2])\n",
    "main_test(backpropagation, model, verbose=True, data='toy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests passed for packrop implementation when testing with toy (random) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========================  Check gradients ======================== \n",
      "\n",
      " ================= Epoch 1 ================= \n",
      "Gradients consistent with autograd's computations. :) \n",
      "Gradients consistent with finite differences computations. :) \n",
      "\n",
      " ==============  Check that weights have been updated ============= \n",
      "Weights have been updated. :)\n",
      "\n",
      " ===================  Check computational graph =================== \n",
      "All parameters seem correctly attached to the computational graph! :) \n"
     ]
    }
   ],
   "source": [
    "model = MyNet([24*24, 16, 10])\n",
    "main_test(backpropagation, model, verbose=False, data='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests passed for backprop implementation on model when testing with MNIST data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
