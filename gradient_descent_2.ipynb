{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f702f4f",
   "metadata": {},
   "source": [
    "# Section 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe80e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvern\\Anaconda3\\envs\\home\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\kvern\\Anaconda3\\envs\\home\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "device = (torch.device('cpu'))\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915694b",
   "metadata": {},
   "source": [
    "## Define function for loading and preprocessing CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da130b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.8, \n",
    "               data_path='./data/', \n",
    "               preprocessor=None, \n",
    "               seed=123, \n",
    "               keep_labels=['plane', 'bird']):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_val_split : double\n",
    "        Split ratio between train data and validation data.\n",
    "    data_path : str\n",
    "        Path where data is stored. Data is downloaded if is does not already excist\n",
    "    preprocessor : torchvision.transforms.transforms.Compose\n",
    "        Preprocessor for preprocessing data. Default used if none is provided\n",
    "    seed : int\n",
    "        Random seed used in random operations\n",
    "    keep_labels : list\n",
    "        List of image labels we want from cifar10 dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train : torch.utils.data.dataset.Subset\n",
    "        Training data\n",
    "    data_val : torch.utils.data.dataset.Subset\n",
    "        Validation data\n",
    "    data_test : torch.utils.data.dataset.Subset\n",
    "        Testing data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))])\n",
    "    \n",
    "    # Load training and validation data\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    # Split training and testing data\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val], \n",
    "        generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Load testing data\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=False,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # Identify which labels too keep\n",
    "    labels = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, \n",
    "               'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "    final_labels = list(map(labels.get, keep_labels))\n",
    "    \n",
    "    label_map = {}\n",
    "    for i, label in enumerate(final_labels):\n",
    "        label_map[label] = i\n",
    "    \n",
    "    # Shave off datasets, only keeping the wanted labels\n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in final_labels]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in final_labels]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in final_labels]\n",
    "    \n",
    "    # Print data set sizes for sanity check\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3075",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29214f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of layers in network\n",
    "        self.L = 4\n",
    "        \n",
    "        # Initialize Zs and As to dictionary\n",
    "        # Z[l] = W[l]A[l-1] + b[l]\n",
    "        # A[l] = g[Z[l]]\n",
    "        self.z = {i:None for i in range(1, self.L+1)}\n",
    "        self.a = {i:None for i in range(self.L+1)}\n",
    "        \n",
    "        '''\n",
    "        Create fully connected (fc) layers\n",
    "        Layers:\n",
    "            n[l0] = 3072\n",
    "            n[l1] = 512\n",
    "            n[l2] = 128\n",
    "            n[l3] = 32\n",
    "            n[l4] = 2\n",
    "        '''\n",
    "        self.fc = nn.ModuleDict({str(i):None for i in range(1, self.L+1)})\n",
    "        self.fc['1'] = nn.Linear(in_features=3072, out_features=512)\n",
    "        self.fc['2'] = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc['3'] = nn.Linear(in_features=128, out_features=32)\n",
    "        self.fc['4'] = nn.Linear(in_features=32, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # First layer\n",
    "        self.z[1] = self.fc['1'](self.a[0])\n",
    "        self.a[1] = torch.relu(self.z[1])\n",
    "        \n",
    "        # Second layer\n",
    "        self.z[2] = self.fc['2'](self.a[1])\n",
    "        self.a[2] = torch.relu(self.z[2])\n",
    "        \n",
    "        # Third layer \n",
    "        self.z[3] = self.fc['3'](self.a[2])\n",
    "        self.a[3] = torch.relu(self.z[3])\n",
    "        \n",
    "        # Fourth layer (output layer)\n",
    "        self.z[4] = self.fc['4'](self.a[3])\n",
    "        self.a[4] = self.z[4]\n",
    "        \n",
    "        return self.a[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152baec",
   "metadata": {},
   "source": [
    "## Define training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94f24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, verbose = True):\n",
    "    '''\n",
    "    Neural network training pipeline using pytorch's SGD\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "    optimizer: torch.optim\n",
    "        Optimization algorithm used for gradient descent\n",
    "    model: nn.Module object\n",
    "        Neural network object being trained\n",
    "    loss_fn: pytorch loss funtion\n",
    "        Loss function for calculating loss\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "        Data loader is an iterable, splitting input data into batches of a given size\n",
    "    verbose: boolean\n",
    "        Set True to print statements\n",
    "        \n",
    "    RETURNS\n",
    "    -------\n",
    "    losses_train: list\n",
    "        List containing loss from each epoch\n",
    "    model: nn.Module object\n",
    "        Trained neural network   \n",
    "    '''\n",
    "    \n",
    "    if verbose:\n",
    "        print(\" --------- Using Pytorch's SGD ---------\")\n",
    "    \n",
    "    # Number of batches\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.double())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch % 5 == 0):\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, weight_decay, momentum, model, loss_fn, train_loader, verbose=True):\n",
    "    '''\n",
    "    Neural network training pipeline using manual gradient descent\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "    lr: double\n",
    "        Learning rate hyperparameter used in gradient descent\n",
    "    weight_decay: double\n",
    "        Weight decay hyperparameter for L2 regularization\n",
    "    momentum: double\n",
    "        Momentum hyperparameter for gradient descent\n",
    "    model: nn.Module object\n",
    "        Neural network object being trained\n",
    "    loss_fn: pytorch loss funtion\n",
    "        Loss function for calculating loss\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "        Data loader is an iterable, splitting input data into batches of a given size\n",
    "    verbose: boolean\n",
    "        Set True to print statements\n",
    "        \n",
    "    RETURNS\n",
    "    -------\n",
    "    losses_train: list\n",
    "        List containing loss from each epoch\n",
    "    model: nn.Module object\n",
    "        Trained neural network   \n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\" --------- Using Manual Update ---------\")\n",
    "    \n",
    "    # Number of batches\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    # List used for momentum calculation in gradient descent\n",
    "    b = [None, None, None, None, None, None, None, None]\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader:\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient at the beginning of new batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.double())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                # Add L2 regularization\n",
    "                grad = p.grad + weight_decay*p.data\n",
    "                # Add momentum\n",
    "                if momentum != 0:\n",
    "                    if b[i] == None:\n",
    "                        b[i] = grad\n",
    "                    else:\n",
    "                        b[i] = momentum*b[i] + grad\n",
    "                    grad = b[i]\n",
    "                # Update parameters\n",
    "                p.data = p.data - lr*grad\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch % 5 == 0):\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b10140",
   "metadata": {},
   "source": [
    "### Function calculating prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b64011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    #TODO: vary good (too good?) validation accuracies seem unlikely? Is there a mistake here or somewhere else?\n",
    "    \"\"\" calculates fraction of predictions that are correct \"\"\"\n",
    "    good_predictions = (y_pred == y_true)\n",
    "    # counting number of true entries in boolean tensor\n",
    "    correct = len(good_predictions.masked_select(good_predictions == True))\n",
    "    acc = correct / len(y_true) # fraction of correct predictions\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f14a3",
   "metadata": {},
   "source": [
    "### Function for getting prediciton labels from given model and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a08bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    output = model(X)\n",
    "    y_pred = output.max(axis=1)[1] # find which label is most likely (0-bird, 1-plane) \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f9bcd",
   "metadata": {},
   "source": [
    "## Set global parameters and load CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfda8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         8007\n",
      "Size of the validation dataset:    1993\n",
      "Size of the test dataset:          2000\n"
     ]
    }
   ],
   "source": [
    "# Set global hyperparameters\n",
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "\n",
    "# Load data\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "\n",
    "# Train loader for training neural network\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Split images and labels into X and y for model evaluation\n",
    "train_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_train), shuffle=False)\n",
    "for data in train_set:\n",
    "    x_train, y_train = data\n",
    "val_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_val), shuffle=False)\n",
    "for data in val_set:\n",
    "    x_validation, y_validation = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553fbef",
   "metadata": {},
   "source": [
    "## Hyperparameter testing\n",
    "Loop for hyperparameter testing and output print statements (made to match ``gradient_descent_output.txt``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08f1594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on device cpu.\n",
      "\n",
      " ----- Global parameters ----- \n",
      "batch_size = 256\n",
      "n_epoch =  30\n",
      "loss_fn =  CrossEntropyLoss()\n",
      "seed =  265\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.000\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "13:19:10.784598  |  Epoch 1  |  Training loss 0.68265\n",
      "13:19:14.946886  |  Epoch 5  |  Training loss 0.56681\n",
      "13:19:20.358093  |  Epoch 10  |  Training loss 0.47583\n",
      "13:19:25.626012  |  Epoch 15  |  Training loss 0.43345\n",
      "13:19:31.151699  |  Epoch 20  |  Training loss 0.39692\n",
      "13:19:37.326982  |  Epoch 25  |  Training loss 0.36463\n",
      "13:19:43.364717  |  Epoch 30  |  Training loss 0.33391\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.87\n",
      "Validation\n",
      "Accuracy: 0.94\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "13:19:45.539759  |  Epoch 1  |  Training loss 0.68265\n",
      "13:19:52.667307  |  Epoch 5  |  Training loss 0.56681\n",
      "13:20:00.647956  |  Epoch 10  |  Training loss 0.47583\n",
      "13:20:08.612673  |  Epoch 15  |  Training loss 0.43345\n",
      "13:20:16.553387  |  Epoch 20  |  Training loss 0.39692\n",
      "13:20:24.700565  |  Epoch 25  |  Training loss 0.36463\n",
      "13:20:32.715806  |  Epoch 30  |  Training loss 0.33391\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.87\n",
      "Validation\n",
      "Accuracy: 0.94\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.010\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "13:20:34.307658  |  Epoch 1  |  Training loss 0.68274\n",
      "13:20:39.783015  |  Epoch 5  |  Training loss 0.57078\n",
      "13:20:46.589896  |  Epoch 10  |  Training loss 0.48054\n",
      "13:20:52.859665  |  Epoch 15  |  Training loss 0.44021\n",
      "13:20:59.248579  |  Epoch 20  |  Training loss 0.40650\n",
      "13:21:05.638490  |  Epoch 25  |  Training loss 0.37690\n",
      "13:21:11.914817  |  Epoch 30  |  Training loss 0.34976\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.86\n",
      "Validation\n",
      "Accuracy: 0.91\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "13:21:13.806727  |  Epoch 1  |  Training loss 0.68274\n",
      "13:21:20.231240  |  Epoch 5  |  Training loss 0.57078\n",
      "13:21:28.410316  |  Epoch 10  |  Training loss 0.48054\n",
      "13:21:37.191378  |  Epoch 15  |  Training loss 0.44021\n",
      "13:21:45.495732  |  Epoch 20  |  Training loss 0.40650\n",
      "13:21:53.685533  |  Epoch 25  |  Training loss 0.37690\n",
      "13:22:02.484262  |  Epoch 30  |  Training loss 0.34976\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.86\n",
      "Validation\n",
      "Accuracy: 0.91\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.90\n",
      "decay: 0.000\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "13:22:04.176766  |  Epoch 1  |  Training loss 0.63020\n",
      "13:22:09.457658  |  Epoch 5  |  Training loss 0.36668\n",
      "13:22:15.563060  |  Epoch 10  |  Training loss 0.22969\n",
      "13:22:21.897453  |  Epoch 15  |  Training loss 0.25548\n",
      "13:22:27.986597  |  Epoch 20  |  Training loss 0.14454\n",
      "13:22:34.056364  |  Epoch 25  |  Training loss 0.10680\n",
      "13:22:40.308889  |  Epoch 30  |  Training loss 0.13038\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.91\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "13:22:42.636231  |  Epoch 1  |  Training loss 0.63020\n",
      "13:22:50.857440  |  Epoch 5  |  Training loss 0.36668\n",
      "13:23:00.686328  |  Epoch 10  |  Training loss 0.22969\n",
      "13:23:10.171493  |  Epoch 15  |  Training loss 0.25548\n",
      "13:23:20.431996  |  Epoch 20  |  Training loss 0.14454\n",
      "13:23:30.889757  |  Epoch 25  |  Training loss 0.10680\n",
      "13:23:40.733730  |  Epoch 30  |  Training loss 0.13038\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.91\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.90\n",
      "decay: 0.010\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "13:23:42.473076  |  Epoch 1  |  Training loss 0.63170\n",
      "13:23:48.069197  |  Epoch 5  |  Training loss 0.37695\n",
      "13:23:54.899955  |  Epoch 10  |  Training loss 0.25715\n",
      "13:24:02.526576  |  Epoch 15  |  Training loss 0.18491\n",
      "13:24:09.429675  |  Epoch 20  |  Training loss 0.13795\n",
      "13:24:16.148726  |  Epoch 25  |  Training loss 0.18265\n",
      "13:24:23.622763  |  Epoch 30  |  Training loss 0.15289\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.94\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "13:24:26.041066  |  Epoch 1  |  Training loss 0.63170\n",
      "13:24:34.301545  |  Epoch 5  |  Training loss 0.37695\n",
      "13:24:43.796815  |  Epoch 10  |  Training loss 0.25715\n",
      "13:24:53.175931  |  Epoch 15  |  Training loss 0.18491\n",
      "13:25:02.607866  |  Epoch 20  |  Training loss 0.13795\n",
      "13:25:11.968610  |  Epoch 25  |  Training loss 0.18265\n",
      "13:25:21.360530  |  Epoch 30  |  Training loss 0.15289\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.94\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.80\n",
      "decay: 0.001\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "13:25:23.032623  |  Epoch 1  |  Training loss 0.64847\n",
      "13:25:28.463392  |  Epoch 5  |  Training loss 0.41053\n",
      "13:25:35.325063  |  Epoch 10  |  Training loss 0.30187\n",
      "13:25:42.126323  |  Epoch 15  |  Training loss 0.19337\n",
      "13:25:49.077280  |  Epoch 20  |  Training loss 0.15198\n",
      "13:25:55.831869  |  Epoch 25  |  Training loss 0.11300\n",
      "13:26:02.556960  |  Epoch 30  |  Training loss 0.15269\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.93\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "13:26:04.771061  |  Epoch 1  |  Training loss 0.64847\n",
      "13:26:12.285749  |  Epoch 5  |  Training loss 0.41053\n",
      "13:26:21.838377  |  Epoch 10  |  Training loss 0.30187\n",
      "13:26:31.225308  |  Epoch 15  |  Training loss 0.19337\n",
      "13:26:40.872932  |  Epoch 20  |  Training loss 0.15198\n",
      "13:26:50.670980  |  Epoch 25  |  Training loss 0.11300\n",
      "13:27:00.216639  |  Epoch 30  |  Training loss 0.15269\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.93\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nTraining on device {str(device)}.\\n')\n",
    "\n",
    "print(' ----- Global parameters ----- ')\n",
    "print('batch_size = %d'%batch_size)\n",
    "print('n_epoch =  %d'%n_epochs)\n",
    "print('loss_fn =  CrossEntropyLoss()')\n",
    "print('seed =  %d'%seed)\n",
    "\n",
    "# For storing which MLP does the best\n",
    "best_acc = 0\n",
    "val_accs = []\n",
    "\n",
    "# Parameters to test (same as from output text)\n",
    "moms = [0, 0, 0.9, 0.9, 0.8]\n",
    "decays = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "\n",
    "# use list of models to avoid having to train them so much? \n",
    "# not sure how this will work with all the pytorch subclass stuff\n",
    "\n",
    "for i in range(len(moms)):\n",
    "    mom = moms[i]\n",
    "    decay = decays[i]\n",
    "    \n",
    "    print('\\n =========================================================\\n  Current parameters:')\n",
    "    print('lr: %.2f'%lr)\n",
    "    print('mom: %.2f'%mom)\n",
    "    print('decay: %.3f\\n'%decay)\n",
    "    \n",
    "    # Training Pytorch's SGD MLP (model1)\n",
    "    torch.manual_seed(seed)\n",
    "    model1 = MyMLP()\n",
    "    optimizer = torch.optim.SGD(model1.parameters(), lr=lr, weight_decay=decay, momentum=mom)\n",
    "    losses_train1, model1 = train(n_epochs, optimizer, model1, loss_fn, train_loader)\n",
    "    \n",
    "    # Get predictions on whole training set\n",
    "    y_pred_train = predict(model1, x_train)\n",
    "    \n",
    "    # Calculating accuracies from training predictions\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # Get predictions on whole validation set\n",
    "    y_pred_val = predict(model1, x_validation)\n",
    "    \n",
    "    \n",
    "    # Calculating accuracies from validation predictions\n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    \n",
    "    # Update best parameters if current validation accuracy is higher or equal to current best accuracy\n",
    "    if acc_v >= best_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'PyTorch SDG'\n",
    "        best_acc = acc_v\n",
    "    \n",
    "    # Print training and validation accuracies\n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "    \n",
    "    # Training manual update MLP (model2)\n",
    "    torch.manual_seed(seed)\n",
    "    model2 = MyMLP()\n",
    "    losses_train2, model2 = train_manual_update(n_epochs, lr, decay, mom, model2, loss_fn, train_loader)\n",
    "    \n",
    "    # Get predictions on whole training set\n",
    "    y_pred_train = predict(model2, x_train)\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    y_pred_val = predict(model2, x_validation)\n",
    "    \n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    val_accs.append(acc_v)\n",
    "    if acc_v >= best_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'Manual update'\n",
    "        best_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c25e7",
   "metadata": {},
   "source": [
    "Questions 9 and 10:\n",
    "Select the best model among those trained in the previous question based on their accuracy. Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6230874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Best model ---\n",
      "Learning rate: 0.01\n",
      "Momentum: 0.80\n",
      "Decay: 0.001\n",
      "SDG method: Manual update\n",
      "Validation accuracy: 1.00\n",
      "\n",
      "Test accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Print best results from validation set\n",
    "print('\\n --- Best model ---')\n",
    "print('Learning rate: %.2f'%lr)\n",
    "print('Momentum: %.2f'%best_mom)\n",
    "print('Decay: %.3f'%best_decay)\n",
    "print('SDG method: ' + best_SDG)\n",
    "print('Validation accuracy: %.2f\\n'%best_acc)\n",
    "\n",
    "\n",
    "test_set = torch.utils.data.DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n",
    "for data in test_set:\n",
    "    x_test, y_test = data\n",
    "    \n",
    "model = MyMLP()\n",
    "if best_SDG == 'PyTorch SDG':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=best_decay, momentum=best_mom)\n",
    "    losses_train, model = train(n_epochs, optimizer, model, loss_fn, train_loader, verbose=False)\n",
    "else:\n",
    "    losses_train, model = train_manual_update(n_epochs, lr, best_decay, best_mom, model, loss_fn, train_loader, verbose=False)\n",
    "\n",
    "y_pred_test = predict(model, x_test.double())\n",
    "\n",
    "acc_test = calculate_accuracy(y_pred_test, y_test)\n",
    "\n",
    "print('Test accuracy: %.2f'%acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5202eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = [i for i in range(n_epochs)]\n",
    "plt.plot(epochs, losses_train)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a86a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
