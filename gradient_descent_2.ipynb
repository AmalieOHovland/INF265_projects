{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f702f4f",
   "metadata": {},
   "source": [
    "# Section 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27770f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvern\\Anaconda3\\envs\\home\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\kvern\\Anaconda3\\envs\\home\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "device = (torch.device('cpu'))\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93130d43",
   "metadata": {},
   "source": [
    "## Define function for loading and preprocessing CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e011e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, \n",
    "               data_path='./data/', \n",
    "               preprocessor=None, \n",
    "               seed=123, \n",
    "               keep_labels=['plane', 'bird']):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_val_split : double\n",
    "        Split ratio between train data and validation data.\n",
    "    data_path : str\n",
    "        Path where data is stored. Data is downloaded if is does not already excist\n",
    "    preprocessor : torchvision.transforms.transforms.Compose\n",
    "        Preprocessor for preprocessing data. Default used if none is provided\n",
    "    seed : int\n",
    "        Random seed used in random operations\n",
    "    keep_labels : list\n",
    "        List of image labels we want from cifar10 dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train : torch.utils.data.dataset.Subset\n",
    "        Training data\n",
    "    data_val : torch.utils.data.dataset.Subset\n",
    "        Validation data\n",
    "    data_test : torch.utils.data.dataset.Subset\n",
    "        Testing data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))])\n",
    "    \n",
    "    # Load training and validation data\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    # Split training and testing data\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val], \n",
    "        generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Load testing data\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=False,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # Identify which labels too keep\n",
    "    labels = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, \n",
    "               'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "    final_labels = list(map(labels.get, keep_labels))\n",
    "    \n",
    "    label_map = {}\n",
    "    for i, label in enumerate(final_labels):\n",
    "        label_map[label] = i\n",
    "    \n",
    "    # Shave off datasets, only keeping the wanted labels\n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in final_labels]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in final_labels]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in final_labels]\n",
    "    \n",
    "    # Print data set sizes for sanity check\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc26a0",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6928ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of layers in network\n",
    "        self.L = 4\n",
    "        \n",
    "        # Initialize Zs and As to dictionary\n",
    "        # Z[l] = W[l]A[l-1] + b[l]\n",
    "        # A[l] = g[Z[l]]\n",
    "        self.z = {i:None for i in range(1, self.L+1)}\n",
    "        self.a = {i:None for i in range(self.L+1)}\n",
    "        \n",
    "        '''\n",
    "        Create fully connected (fc) layers\n",
    "        Layers:\n",
    "            n[l0] = 3072\n",
    "            n[l1] = 512\n",
    "            n[l2] = 128\n",
    "            n[l3] = 32\n",
    "            n[l4] = 2\n",
    "        '''\n",
    "        self.fc = nn.ModuleDict({str(i):None for i in range(1, self.L+1)})\n",
    "        self.fc['1'] = nn.Linear(in_features=3072, out_features=512)\n",
    "        self.fc['2'] = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc['3'] = nn.Linear(in_features=128, out_features=32)\n",
    "        self.fc['4'] = nn.Linear(in_features=32, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # First layer\n",
    "        self.z[1] = self.fc['1'](self.a[0])\n",
    "        self.a[1] = torch.relu(self.z[1])\n",
    "        \n",
    "        # Second layer\n",
    "        self.z[2] = self.fc['2'](self.a[1])\n",
    "        self.a[2] = torch.relu(self.z[2])\n",
    "        \n",
    "        # Third layer \n",
    "        self.z[3] = self.fc['3'](self.a[2])\n",
    "        self.a[3] = torch.relu(self.z[3])\n",
    "        \n",
    "        # Fourth layer (output layer)\n",
    "        self.z[4] = self.fc['4'](self.a[3])\n",
    "        self.a[4] = self.z[4]\n",
    "        \n",
    "        return self.a[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b92fb8",
   "metadata": {},
   "source": [
    "## Define training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c20759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn):\n",
    "    # Neural network training pipeline using pytorch's SGD\n",
    "    \n",
    "    print(\" --------- Using Pytorch's SGD ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, weight_decay, momentum, model, loss_fn):\n",
    "    # Neural network training pipeline using manual gradient descent\n",
    "    \n",
    "    print(\" --------- Using Manual Update ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    b = [None, None, None, None, None, None, None, None]\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                # Add L2 regularization\n",
    "                grad = p.grad + weight_decay*p.data\n",
    "                # Add momentum\n",
    "                if momentum != 0:\n",
    "                    if b[i] == None:\n",
    "                        b[i] = grad\n",
    "                    else:\n",
    "                        b[i] = momentum*b[i] + grad\n",
    "                    grad = b[i]\n",
    "                \n",
    "                p.data = p.data - lr*grad\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d098",
   "metadata": {},
   "source": [
    "Loop for hyperparameter testing and output print statements (made to match ``gradient_descent_output.txt``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08f1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         9017\n",
      "Size of the validation dataset:    983\n",
      "Size of the test dataset:          2000\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3012/91966084.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mlosses_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0my_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not callable"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    \"\"\" calculates fraction of predictions that are correct \"\"\"\n",
    "    good_predictions = (y_pred == y_true)\n",
    "    acc = np.sum(good_predictions) / len(y_true)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# set global hyperparameters\n",
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "\n",
    "# load data\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Split X and y\n",
    "train = torch.utils.data.DataLoader(data_train, batch_size=len(data_train), shuffle=False)\n",
    "for data in train:\n",
    "    x_train, y_train = data\n",
    "val = torch.utils.data.DataLoader(data_train, batch_size=len(data_val), shuffle=False)\n",
    "for data in val:\n",
    "    x_validation, y_validation = data\n",
    "\n",
    "# for storing which MLP does the best\n",
    "current_acc = 0\n",
    "val_accs = []\n",
    "\n",
    "# parameters to test (same as from output text)\n",
    "moms = [0, 0, 0.9, 0.9, 0.8]\n",
    "decays = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "\n",
    "# use list of models to avoid having to train them so much? \n",
    "# not sure how this will work with all the pytorch subclass stuff\n",
    "\n",
    "for i in range(len(moms)):\n",
    "    mom = moms[i]\n",
    "    decay = decays[i]\n",
    "    \n",
    "    print(' =========================================================\\n  Current parameters:')\n",
    "    print('lr: %.2f'%lr)\n",
    "    print('mom: %.2f'%mom)\n",
    "    print('decay: %.2f'%decay)\n",
    "    \n",
    "    # Train your Pytorch's SGD MLP (model1) here and return y_pred_train from last loop of training\n",
    "    model1 = MyMLP()\n",
    "    optimizer = torch.optim.SGD(model1.parameters(), lr=lr, weight_decay=decay, momentum=mom)\n",
    "    losses_train1, model1 = train(n_epochs, optimizer, model1, loss_fn)\n",
    "    y_pred_train = model1(x_train)\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model1(x_validation)\n",
    "    acc_v = calc_acc(y_pred_val, y_validation)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'PyTorch SDG'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('\\nTraining\\nAccuracy: %.2f'%acc_t)\n",
    "    print('\\nValidation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "    \n",
    "    # Train your manual update MLP (model2) here and return y_pred_train from last loop of training\n",
    "    model2 = MyMLP()\n",
    "    losses_train2, model2 = train_manual_update(n_epochs, lr, decay, mom, model2, loss_fn)\n",
    "    y_pred_train = model2(x_train)\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model2(x_validation)\n",
    "    acc_v = calc_acc(y_pred_val, y_validation)\n",
    "    val_accs.append(acc_v)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'manual update'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "\n",
    "# if we make function this could be used?    \n",
    "# best_hyperparams = {'momentum' : best_mom, 'decay' : best_decay, 'SGD version' : best_SDG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c25e7",
   "metadata": {},
   "source": [
    "Questions 9 and 10:\n",
    "Select the best model among those trained in the previous question based on their accuracy. Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n --- Best model ---')\n",
    "print('Learning rate: %.2f'%lr)\n",
    "print('Momentum: %.2f'%best_mom)\n",
    "print('Decay: %.2f'%best_decay)\n",
    "print('Validation accuracy: %.2f'%current_acc)\n",
    "\n",
    "# train the best model? want no output here\n",
    "y_pred_eval = model(x_eval)\n",
    "acc_eval = calculate_accuracy(y_pred_eval, y_eval)\n",
    "\n",
    "print('Test accuracy: %.2f'%acc_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
