{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f702f4f",
   "metadata": {},
   "source": [
    "# Section 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe80e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "device = (torch.device('cpu'))\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915694b",
   "metadata": {},
   "source": [
    "## Define function for loading and preprocessing CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da130b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, \n",
    "               data_path='./data/', \n",
    "               preprocessor=None, \n",
    "               seed=123, \n",
    "               keep_labels=['plane', 'bird']):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_val_split : double\n",
    "        Split ratio between train data and validation data.\n",
    "    data_path : str\n",
    "        Path where data is stored. Data is downloaded if is does not already excist\n",
    "    preprocessor : torchvision.transforms.transforms.Compose\n",
    "        Preprocessor for preprocessing data. Default used if none is provided\n",
    "    seed : int\n",
    "        Random seed used in random operations\n",
    "    keep_labels : list\n",
    "        List of image labels we want from cifar10 dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train : torch.utils.data.dataset.Subset\n",
    "        Training data\n",
    "    data_val : torch.utils.data.dataset.Subset\n",
    "        Validation data\n",
    "    data_test : torch.utils.data.dataset.Subset\n",
    "        Testing data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))])\n",
    "    \n",
    "    # Load training and validation data\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    # Split training and testing data\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val], \n",
    "        generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Load testing data\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=False,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # Identify which labels too keep\n",
    "    labels = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, \n",
    "               'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "    final_labels = list(map(labels.get, keep_labels))\n",
    "    \n",
    "    label_map = {}\n",
    "    for i, label in enumerate(final_labels):\n",
    "        label_map[label] = i\n",
    "    \n",
    "    # Shave off datasets, only keeping the wanted labels\n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in final_labels]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in final_labels]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in final_labels]\n",
    "    \n",
    "    # Print data set sizes for sanity check\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3075",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29214f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of layers in network\n",
    "        self.L = 4\n",
    "        \n",
    "        # Initialize Zs and As to dictionary\n",
    "        # Z[l] = W[l]A[l-1] + b[l]\n",
    "        # A[l] = g[Z[l]]\n",
    "        self.z = {i:None for i in range(1, self.L+1)}\n",
    "        self.a = {i:None for i in range(self.L+1)}\n",
    "        \n",
    "        '''\n",
    "        Create fully connected (fc) layers\n",
    "        Layers:\n",
    "            n[l0] = 3072\n",
    "            n[l1] = 512\n",
    "            n[l2] = 128\n",
    "            n[l3] = 32\n",
    "            n[l4] = 2\n",
    "        '''\n",
    "        self.fc = nn.ModuleDict({str(i):None for i in range(1, self.L+1)})\n",
    "        self.fc['1'] = nn.Linear(in_features=3072, out_features=512)\n",
    "        self.fc['2'] = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc['3'] = nn.Linear(in_features=128, out_features=32)\n",
    "        self.fc['4'] = nn.Linear(in_features=32, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # First layer\n",
    "        self.z[1] = self.fc['1'](self.a[0])\n",
    "        self.a[1] = torch.relu(self.z[1])\n",
    "        \n",
    "        # Second layer\n",
    "        self.z[2] = self.fc['2'](self.a[1])\n",
    "        self.a[2] = torch.relu(self.z[2])\n",
    "        \n",
    "        # Third layer \n",
    "        self.z[3] = self.fc['3'](self.a[2])\n",
    "        self.a[3] = torch.relu(self.z[3])\n",
    "        \n",
    "        # Fourth layer (output layer)\n",
    "        self.z[4] = self.fc['4'](self.a[3])\n",
    "        self.a[4] = self.z[4]\n",
    "        \n",
    "        return self.a[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152baec",
   "metadata": {},
   "source": [
    "## Define training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94f24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn):\n",
    "    # Neural network training pipeline using pytorch's SGD\n",
    "    \n",
    "    print(\" --------- Using Pytorch's SGD ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, weight_decay, momentum, model, loss_fn):\n",
    "    # Neural network training pipeline using manual gradient descent\n",
    "    \n",
    "    print(\" --------- Using Manual Update ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    b = [None, None, None, None, None, None, None, None]\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                # Add L2 regularization\n",
    "                grad = p.grad + weight_decay*p.data\n",
    "                # Add momentum\n",
    "                if momentum != 0:\n",
    "                    if b[i] == None:\n",
    "                        b[i] = grad\n",
    "                    else:\n",
    "                        b[i] = momentum*b[i] + grad\n",
    "                    grad = b[i]\n",
    "                \n",
    "                p.data = p.data - lr*grad\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d098",
   "metadata": {},
   "source": [
    "Loop for hyperparameter testing and output print statements (made to match ``gradient_descent_output.txt``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08f1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         9017\n",
      "Size of the validation dataset:    983\n",
      "Size of the test dataset:          2000\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.00\n",
      " --------- Using Pytorch's SGD ---------\n",
      "20:33:57.163862  |  Epoch 1  |  Training loss 0.69640\n",
      "20:34:03.235625  |  Epoch 5  |  Training loss 0.55943\n",
      "20:34:10.532360  |  Epoch 10  |  Training loss 0.46985\n",
      "20:34:18.474120  |  Epoch 15  |  Training loss 0.42883\n",
      "20:34:26.997328  |  Epoch 20  |  Training loss 0.39156\n",
      "20:34:34.331712  |  Epoch 25  |  Training loss 0.35860\n",
      "20:34:41.037778  |  Epoch 30  |  Training loss 0.32691\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (axis=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: axis, out\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18368/1503725937.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# calculating accuracies from training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0macc_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# calculate accuracies from validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18368/1503725937.py\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[1;34m(y_pred, y_true)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\" calculates fraction of predictions that are correct \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgood_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgood_predictions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2259\u001b[1;33m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[0;32m   2260\u001b[0m                           initial=initial, where=where)\n\u001b[0;32m   2261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (axis=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: axis, out\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    \"\"\" calculates fraction of predictions that are correct \"\"\"\n",
    "    good_predictions = (y_pred == y_true)\n",
    "    acc = np.sum(good_predictions) / len(y_true)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# set global hyperparameters\n",
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "\n",
    "# load data\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Split X and y\n",
    "train_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_train), shuffle=False)\n",
    "for data in train_set:\n",
    "    x_train, y_train = data\n",
    "val_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_val), shuffle=False)\n",
    "for data in val_set:\n",
    "    x_validation, y_validation = data\n",
    "\n",
    "# for storing which MLP does the best\n",
    "current_acc = 0\n",
    "val_accs = []\n",
    "\n",
    "# parameters to test (same as from output text)\n",
    "moms = [0, 0, 0.9, 0.9, 0.8]\n",
    "decays = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "\n",
    "# use list of models to avoid having to train them so much? \n",
    "# not sure how this will work with all the pytorch subclass stuff\n",
    "\n",
    "for i in range(len(moms)):\n",
    "    mom = moms[i]\n",
    "    decay = decays[i]\n",
    "    \n",
    "    print(' =========================================================\\n  Current parameters:')\n",
    "    print('lr: %.2f'%lr)\n",
    "    print('mom: %.2f'%mom)\n",
    "    print('decay: %.2f'%decay)\n",
    "    \n",
    "    # Train your Pytorch's SGD MLP (model1) here and return y_pred_train from last loop of training\n",
    "    model1 = MyMLP()\n",
    "    optimizer = torch.optim.SGD(model1.parameters(), lr=lr, weight_decay=decay, momentum=mom)\n",
    "    losses_train1, model1 = train(n_epochs, optimizer, model1, loss_fn)\n",
    "    y_pred_train = model1(x_train)\n",
    "    y_pred_train = y_pred_train.max(axis=1)[1]\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model1(x_validation)\n",
    "    y_pred_val = y_pred_val.max(axis=1)[1]\n",
    "    acc_v = calcuate_accuracy(y_pred_val, y_validation)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'PyTorch SDG'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('\\nTraining\\nAccuracy: %.2f'%acc_t)\n",
    "    print('\\nValidation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "    \n",
    "    # Train your manual update MLP (model2) here and return y_pred_train from last loop of training\n",
    "    model2 = MyMLP()\n",
    "    losses_train2, model2 = train_manual_update(n_epochs, lr, decay, mom, model2, loss_fn)\n",
    "    y_pred_train = model2(x_train)\n",
    "    y_pred_train = y_pred_train.max(axis=1)[1]\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model2(x_validation)\n",
    "    y_pred_val = y_pred_val.max(axis=1)[1]\n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    val_accs.append(acc_v)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'manual update'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "\n",
    "# if we make function this could be used?    \n",
    "# best_hyperparams = {'momentum' : best_mom, 'decay' : best_decay, 'SGD version' : best_SDG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c25e7",
   "metadata": {},
   "source": [
    "Questions 9 and 10:\n",
    "Select the best model among those trained in the previous question based on their accuracy. Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n --- Best model ---')\n",
    "print('Learning rate: %.2f'%lr)\n",
    "print('Momentum: %.2f'%best_mom)\n",
    "print('Decay: %.2f'%best_decay)\n",
    "print('Validation accuracy: %.2f'%current_acc)\n",
    "\n",
    "# train the best model? want no output here\n",
    "y_pred_eval = model(x_eval)\n",
    "acc_eval = calculate_accuracy(y_pred_eval, y_eval)\n",
    "\n",
    "print('Test accuracy: %.2f'%acc_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
