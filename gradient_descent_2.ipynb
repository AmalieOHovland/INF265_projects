{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f702f4f",
   "metadata": {},
   "source": [
    "# Section 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe80e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "device = (torch.device('cpu'))\n",
    "seed = 123\n",
    "#TODO: details: seed in example is \n",
    "#seed = 265 \n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915694b",
   "metadata": {},
   "source": [
    "## Define function for loading and preprocessing CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da130b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, \n",
    "               data_path='./data/', \n",
    "               preprocessor=None, \n",
    "               seed=123, \n",
    "               keep_labels=['plane', 'bird']):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_val_split : double\n",
    "        Split ratio between train data and validation data.\n",
    "    data_path : str\n",
    "        Path where data is stored. Data is downloaded if is does not already excist\n",
    "    preprocessor : torchvision.transforms.transforms.Compose\n",
    "        Preprocessor for preprocessing data. Default used if none is provided\n",
    "    seed : int\n",
    "        Random seed used in random operations\n",
    "    keep_labels : list\n",
    "        List of image labels we want from cifar10 dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train : torch.utils.data.dataset.Subset\n",
    "        Training data\n",
    "    data_val : torch.utils.data.dataset.Subset\n",
    "        Validation data\n",
    "    data_test : torch.utils.data.dataset.Subset\n",
    "        Testing data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))])\n",
    "    \n",
    "    # Load training and validation data\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    # Split training and testing data\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val], \n",
    "        generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Load testing data\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=False,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # Identify which labels too keep\n",
    "    labels = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, \n",
    "               'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "    final_labels = list(map(labels.get, keep_labels))\n",
    "    \n",
    "    label_map = {}\n",
    "    for i, label in enumerate(final_labels):\n",
    "        label_map[label] = i\n",
    "    \n",
    "    # Shave off datasets, only keeping the wanted labels\n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in final_labels]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in final_labels]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in final_labels]\n",
    "    \n",
    "    # Print data set sizes for sanity check\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3075",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29214f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of layers in network\n",
    "        self.L = 4\n",
    "        \n",
    "        # Initialize Zs and As to dictionary\n",
    "        # Z[l] = W[l]A[l-1] + b[l]\n",
    "        # A[l] = g[Z[l]]\n",
    "        self.z = {i:None for i in range(1, self.L+1)}\n",
    "        self.a = {i:None for i in range(self.L+1)}\n",
    "        \n",
    "        '''\n",
    "        Create fully connected (fc) layers\n",
    "        Layers:\n",
    "            n[l0] = 3072\n",
    "            n[l1] = 512\n",
    "            n[l2] = 128\n",
    "            n[l3] = 32\n",
    "            n[l4] = 2\n",
    "        '''\n",
    "        self.fc = nn.ModuleDict({str(i):None for i in range(1, self.L+1)})\n",
    "        self.fc['1'] = nn.Linear(in_features=3072, out_features=512)\n",
    "        self.fc['2'] = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc['3'] = nn.Linear(in_features=128, out_features=32)\n",
    "        self.fc['4'] = nn.Linear(in_features=32, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # First layer\n",
    "        self.z[1] = self.fc['1'](self.a[0])\n",
    "        self.a[1] = torch.relu(self.z[1])\n",
    "        \n",
    "        # Second layer\n",
    "        self.z[2] = self.fc['2'](self.a[1])\n",
    "        self.a[2] = torch.relu(self.z[2])\n",
    "        \n",
    "        # Third layer \n",
    "        self.z[3] = self.fc['3'](self.a[2])\n",
    "        self.a[3] = torch.relu(self.z[3])\n",
    "        \n",
    "        # Fourth layer (output layer)\n",
    "        self.z[4] = self.fc['4'](self.a[3])\n",
    "        self.a[4] = self.z[4]\n",
    "        \n",
    "        return self.a[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152baec",
   "metadata": {},
   "source": [
    "## Define training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d94f24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, verbose = True):\n",
    "    # Neural network training pipeline using pytorch's SGD\n",
    "    \n",
    "    if verbose:\n",
    "        print(\" --------- Using Pytorch's SGD ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.double())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch % 5 == 0):\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, weight_decay, momentum, model, loss_fn, train_loader, verbose=True):\n",
    "    # Neural network training pipeline using manual gradient descent\n",
    "    if verbose:\n",
    "        print(\" --------- Using Manual Update ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    b = [None, None, None, None, None, None, None, None]\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.double())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                # Add L2 regularization\n",
    "                grad = p.grad + weight_decay*p.data\n",
    "                # Add momentum\n",
    "                if momentum != 0:\n",
    "                    if b[i] == None:\n",
    "                        b[i] = grad\n",
    "                    else:\n",
    "                        b[i] = momentum*b[i] + grad\n",
    "                    grad = b[i]\n",
    "                \n",
    "                p.data = p.data - lr*grad\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch % 5 == 0):\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d098",
   "metadata": {},
   "source": [
    "Loop for hyperparameter testing and output print statements (made to match ``gradient_descent_output.txt``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08f1594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         9017\n",
      "Size of the validation dataset:    983\n",
      "Size of the test dataset:          2000\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.00\n",
      " --------- Using Pytorch's SGD ---------\n",
      "22:04:08.534610  |  Epoch 1  |  Training loss 0.69640\n",
      "22:04:20.039313  |  Epoch 5  |  Training loss 0.55943\n",
      "22:04:34.364872  |  Epoch 10  |  Training loss 0.46984\n",
      "22:04:48.680984  |  Epoch 15  |  Training loss 0.42884\n",
      "22:05:03.009240  |  Epoch 20  |  Training loss 0.39155\n",
      "22:05:17.368357  |  Epoch 25  |  Training loss 0.35858\n",
      "22:05:31.716179  |  Epoch 30  |  Training loss 0.32692\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.87\n",
      "Validation\n",
      "Accuracy: 0.89\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "22:05:36.100846  |  Epoch 1  |  Training loss 0.68580\n",
      "22:05:47.996075  |  Epoch 5  |  Training loss 0.56936\n",
      "22:06:02.859236  |  Epoch 10  |  Training loss 0.47856\n",
      "22:06:17.713277  |  Epoch 15  |  Training loss 0.43433\n",
      "22:06:32.603318  |  Epoch 20  |  Training loss 0.39692\n",
      "22:06:47.461092  |  Epoch 25  |  Training loss 0.36451\n",
      "22:07:02.317333  |  Epoch 30  |  Training loss 0.33349\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.87\n",
      "Validation\n",
      "Accuracy: 0.89\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.01\n",
      " --------- Using Pytorch's SGD ---------\n",
      "22:07:06.598548  |  Epoch 1  |  Training loss 0.67802\n",
      "22:07:18.240376  |  Epoch 5  |  Training loss 0.56513\n",
      "22:07:32.904133  |  Epoch 10  |  Training loss 0.48401\n",
      "22:07:47.692604  |  Epoch 15  |  Training loss 0.44285\n",
      "22:08:02.267999  |  Epoch 20  |  Training loss 0.40797\n",
      "22:08:16.964932  |  Epoch 25  |  Training loss 0.37835\n",
      "22:08:31.560193  |  Epoch 30  |  Training loss 0.35132\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.86\n",
      "Validation\n",
      "Accuracy: 0.89\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "22:08:35.940446  |  Epoch 1  |  Training loss 0.66961\n",
      "22:08:47.955108  |  Epoch 5  |  Training loss 0.54663\n",
      "22:09:02.962842  |  Epoch 10  |  Training loss 0.47725\n",
      "22:09:17.915522  |  Epoch 15  |  Training loss 0.43663\n",
      "22:09:32.895761  |  Epoch 20  |  Training loss 0.40228\n",
      "22:09:47.828655  |  Epoch 25  |  Training loss 0.37230\n",
      "22:10:02.736635  |  Epoch 30  |  Training loss 0.34457\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.86\n",
      "Validation\n",
      "Accuracy: 0.89\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.90\n",
      "decay: 0.00\n",
      " --------- Using Pytorch's SGD ---------\n",
      "22:10:07.044805  |  Epoch 1  |  Training loss 0.61383\n",
      "22:10:18.721302  |  Epoch 5  |  Training loss 0.36492\n",
      "22:10:33.355191  |  Epoch 10  |  Training loss 0.21153\n",
      "22:10:47.946966  |  Epoch 15  |  Training loss 0.14048\n",
      "22:11:02.541451  |  Epoch 20  |  Training loss 0.13526\n",
      "22:11:17.127814  |  Epoch 25  |  Training loss 0.16508\n",
      "22:11:31.750046  |  Epoch 30  |  Training loss 0.11156\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.99\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "22:11:36.192517  |  Epoch 1  |  Training loss 0.61674\n",
      "22:11:48.470703  |  Epoch 5  |  Training loss 0.35812\n",
      "22:12:03.803269  |  Epoch 10  |  Training loss 0.22186\n",
      "22:12:19.196204  |  Epoch 15  |  Training loss 0.21689\n",
      "22:12:34.593287  |  Epoch 20  |  Training loss 0.08385\n",
      "22:12:49.932683  |  Epoch 25  |  Training loss 0.13077\n",
      "22:13:05.254048  |  Epoch 30  |  Training loss 0.09555\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.96\n",
      "Validation\n",
      "Accuracy: 1.00\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.90\n",
      "decay: 0.01\n",
      " --------- Using Pytorch's SGD ---------\n",
      "22:13:09.584454  |  Epoch 1  |  Training loss 0.60569\n",
      "22:13:21.461675  |  Epoch 5  |  Training loss 0.37070\n",
      "22:13:36.246042  |  Epoch 10  |  Training loss 0.26934\n",
      "22:13:51.054120  |  Epoch 15  |  Training loss 0.16947\n",
      "22:14:05.868990  |  Epoch 20  |  Training loss 0.14428\n",
      "22:14:20.702236  |  Epoch 25  |  Training loss 0.14119\n",
      "22:14:35.672026  |  Epoch 30  |  Training loss 0.09675\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.94\n",
      "Validation\n",
      "Accuracy: 0.96\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "22:14:40.148894  |  Epoch 1  |  Training loss 0.60922\n",
      "22:14:52.954138  |  Epoch 5  |  Training loss 0.37594\n",
      "22:15:08.405657  |  Epoch 10  |  Training loss 0.27157\n",
      "22:15:24.031249  |  Epoch 15  |  Training loss 0.20290\n",
      "22:15:39.416336  |  Epoch 20  |  Training loss 0.16241\n",
      "22:15:54.857859  |  Epoch 25  |  Training loss 0.14947\n",
      "22:16:10.155666  |  Epoch 30  |  Training loss 0.11725\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.97\n",
      "Validation\n",
      "Accuracy: 0.99\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.80\n",
      "decay: 0.00\n",
      " --------- Using Pytorch's SGD ---------\n",
      "22:16:14.509956  |  Epoch 1  |  Training loss 0.62840\n",
      "22:16:26.411836  |  Epoch 5  |  Training loss 0.39623\n",
      "22:16:41.220187  |  Epoch 10  |  Training loss 0.29254\n",
      "22:16:56.061441  |  Epoch 15  |  Training loss 0.18813\n",
      "22:17:10.935343  |  Epoch 20  |  Training loss 0.14722\n",
      "22:17:25.739459  |  Epoch 25  |  Training loss 0.16027\n",
      "22:17:40.523606  |  Epoch 30  |  Training loss 0.07078\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.91\n",
      "Validation\n",
      "Accuracy: 0.94\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "22:17:44.961020  |  Epoch 1  |  Training loss 0.64353\n",
      "22:17:57.268615  |  Epoch 5  |  Training loss 0.40741\n",
      "22:18:12.625495  |  Epoch 10  |  Training loss 0.30360\n",
      "22:18:27.947274  |  Epoch 15  |  Training loss 0.20044\n",
      "22:18:43.384739  |  Epoch 20  |  Training loss 0.14292\n",
      "22:18:58.802283  |  Epoch 25  |  Training loss 0.18007\n",
      "22:19:14.419993  |  Epoch 30  |  Training loss 0.07306\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.88\n",
      "Validation\n",
      "Accuracy: 0.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    #TODO: vary good (too good?) validation accuracies seem unlikely? Is there a mistake here or somewhere else?\n",
    "    \"\"\" calculates fraction of predictions that are correct \"\"\"\n",
    "    good_predictions = (y_pred == y_true)\n",
    "    # counting number of true entries in boolean tensor\n",
    "    correct = len(good_predictions.masked_select(good_predictions == True))\n",
    "    acc = correct / len(y_true) # fraction of correct predictions\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# set global hyperparameters\n",
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "\n",
    "# load data\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Split X and y\n",
    "train_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_train), shuffle=False)\n",
    "for data in train_set:\n",
    "    x_train, y_train = data\n",
    "val_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_val), shuffle=False)\n",
    "for data in val_set:\n",
    "    x_validation, y_validation = data\n",
    "\n",
    "print('\\nTraining on device cpu.')\n",
    "\n",
    "print('   Global parameters:')\n",
    "print('batch_size = %d'%batch_size)\n",
    "print('n_epoch =  %d'%n_epoch)\n",
    "print('loss_fn =  CrossEntropyLoss()')\n",
    "print('seed =  %d'%seed)\n",
    "# for storing which MLP does the best\n",
    "current_acc = 0\n",
    "val_accs = []\n",
    "\n",
    "# parameters to test (same as from output text)\n",
    "moms = [0, 0, 0.9, 0.9, 0.8]\n",
    "decays = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "\n",
    "# use list of models to avoid having to train them so much? \n",
    "# not sure how this will work with all the pytorch subclass stuff\n",
    "\n",
    "for i in range(len(moms)):\n",
    "    mom = moms[i]\n",
    "    decay = decays[i]\n",
    "    \n",
    "    print('\\n =========================================================\\n  Current parameters:')\n",
    "    print('lr: %.2f'%lr)\n",
    "    print('mom: %.2f'%mom)\n",
    "    print('decay: %.2f\\n'%decay)\n",
    "    \n",
    "    # Train your Pytorch's SGD MLP (model1) here and return y_pred_train from last loop of training\n",
    "    model1 = MyMLP()\n",
    "    optimizer = torch.optim.SGD(model1.parameters(), lr=lr, weight_decay=decay, momentum=mom)\n",
    "    losses_train1, model1 = train(n_epochs, optimizer, model1, loss_fn, train_loader)\n",
    "    y_pred_train = model1(x_train.double())\n",
    "    y_pred_train = y_pred_train.max(axis=1)[1]\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model1(x_validation.double())\n",
    "    y_pred_val = y_pred_val.max(axis=1)[1]\n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'PyTorch SDG'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "    \n",
    "    # Train your manual update MLP (model2) here and return y_pred_train from last loop of training\n",
    "    model2 = MyMLP()\n",
    "    losses_train2, model2 = train_manual_update(n_epochs, lr, decay, mom, model2, loss_fn, train_loader)\n",
    "    y_pred_train = model2(x_train.double())\n",
    "    y_pred_train = y_pred_train.max(axis=1)[1]\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model2(x_validation.double())\n",
    "    y_pred_val = y_pred_val.max(axis=1)[1]\n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    val_accs.append(acc_v)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'Manual update'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "\n",
    "# if we make function this could be used?    \n",
    "# best_hyperparams = {'momentum' : best_mom, 'decay' : best_decay, 'SGD version' : best_SDG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c25e7",
   "metadata": {},
   "source": [
    "Questions 9 and 10:\n",
    "Select the best model among those trained in the previous question based on their accuracy. Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6230874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Best model ---\n",
      "Learning rate: 0.01\n",
      "Momentum: 0.90\n",
      "Decay: 0.00\n",
      "SDG method: manual update\n",
      "Validation accuracy: 1.00\n",
      "\n",
      "Test accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "print('\\n --- Best model ---')\n",
    "print('Learning rate: %.2f'%lr)\n",
    "print('Momentum: %.2f'%best_mom)\n",
    "print('Decay: %.2f'%best_decay)\n",
    "print('SDG method: ' + best_SDG)\n",
    "print('Validation accuracy: %.2f\\n'%current_acc)\n",
    "\n",
    "test_set = torch.utils.data.DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n",
    "for data in test_set:\n",
    "    x_test, y_test = data\n",
    "\n",
    "model = MyMLP()\n",
    "if best_SDG == 'PyTorch SDG':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=best_decay, momentum=best_mom)\n",
    "    losses_train, model = train(n_epochs, optimizer, model, loss_fn, train_loader, verbose=False)\n",
    "else:\n",
    "    losses_train, model = train_manual_update(n_epochs, lr, best_decay, best_mom, model, loss_fn, train_loader, verbose=False)\n",
    "y_pred_test = model(x_test.double())\n",
    "y_pred_test = y_pred_test.max(axis=1)[1]\n",
    "acc_test = calculate_accuracy(y_pred_test, y_test)\n",
    "\n",
    "print('Test accuracy: %.2f'%acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5202eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = [i for i in range(n_epochs)]\n",
    "plt.plot(epochs, losses_train)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
