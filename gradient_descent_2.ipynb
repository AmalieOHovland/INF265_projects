{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f702f4f",
   "metadata": {},
   "source": [
    "# Section 3: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe80e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "\n",
    "device = (torch.device('cpu'))\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915694b",
   "metadata": {},
   "source": [
    "## Define function for loading and preprocessing CIFAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da130b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, \n",
    "               data_path='./data/', \n",
    "               preprocessor=None, \n",
    "               seed=123, \n",
    "               keep_labels=['plane', 'bird']):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_val_split : double\n",
    "        Split ratio between train data and validation data.\n",
    "    data_path : str\n",
    "        Path where data is stored. Data is downloaded if is does not already excist\n",
    "    preprocessor : torchvision.transforms.transforms.Compose\n",
    "        Preprocessor for preprocessing data. Default used if none is provided\n",
    "    seed : int\n",
    "        Random seed used in random operations\n",
    "    keep_labels : list\n",
    "        List of image labels we want from cifar10 dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train : torch.utils.data.dataset.Subset\n",
    "        Training data\n",
    "    data_val : torch.utils.data.dataset.Subset\n",
    "        Validation data\n",
    "    data_test : torch.utils.data.dataset.Subset\n",
    "        Testing data\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Define preprocessor if not already given\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))])\n",
    "    \n",
    "    # Load training and validation data\n",
    "    data_train_val = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "    \n",
    "    # Split training and testing data\n",
    "    n_train = int(len(data_train_val)*train_val_split)\n",
    "    n_val =  len(data_train_val) - n_train\n",
    "\n",
    "    data_train, data_val = random_split(\n",
    "        data_train_val, \n",
    "        [n_train, n_val], \n",
    "        generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Load testing data\n",
    "    data_test = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=False,      \n",
    "        download=True,\n",
    "        transform=preprocessor)\n",
    "\n",
    "    # Identify which labels too keep\n",
    "    labels = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, \n",
    "               'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "    final_labels = list(map(labels.get, keep_labels))\n",
    "    \n",
    "    label_map = {}\n",
    "    for i, label in enumerate(final_labels):\n",
    "        label_map[label] = i\n",
    "    \n",
    "    # Shave off datasets, only keeping the wanted labels\n",
    "    data_train = [(img, label_map[label]) for img, label in data_train if label in final_labels]\n",
    "    data_val = [(img, label_map[label]) for img, label in data_val if label in final_labels]\n",
    "    data_test = [(img, label_map[label]) for img, label in data_test if label in final_labels]\n",
    "    \n",
    "    # Print data set sizes for sanity check\n",
    "    print(\"Size of the train dataset:        \", len(data_train))\n",
    "    print(\"Size of the validation dataset:   \", len(data_val))\n",
    "    print(\"Size of the test dataset:         \", len(data_test))\n",
    "    \n",
    "    return data_train, data_val, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e3075",
   "metadata": {},
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29214f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of layers in network\n",
    "        self.L = 4\n",
    "        \n",
    "        # Initialize Zs and As to dictionary\n",
    "        # Z[l] = W[l]A[l-1] + b[l]\n",
    "        # A[l] = g[Z[l]]\n",
    "        self.z = {i:None for i in range(1, self.L+1)}\n",
    "        self.a = {i:None for i in range(self.L+1)}\n",
    "        \n",
    "        '''\n",
    "        Create fully connected (fc) layers\n",
    "        Layers:\n",
    "            n[l0] = 3072\n",
    "            n[l1] = 512\n",
    "            n[l2] = 128\n",
    "            n[l3] = 32\n",
    "            n[l4] = 2\n",
    "        '''\n",
    "        self.fc = nn.ModuleDict({str(i):None for i in range(1, self.L+1)})\n",
    "        self.fc['1'] = nn.Linear(in_features=3072, out_features=512)\n",
    "        self.fc['2'] = nn.Linear(in_features=512, out_features=128)\n",
    "        self.fc['3'] = nn.Linear(in_features=128, out_features=32)\n",
    "        self.fc['4'] = nn.Linear(in_features=32, out_features=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Input layer\n",
    "        self.a[0] = torch.flatten(x, 1)\n",
    "        \n",
    "        # First layer\n",
    "        self.z[1] = self.fc['1'](self.a[0])\n",
    "        self.a[1] = torch.relu(self.z[1])\n",
    "        \n",
    "        # Second layer\n",
    "        self.z[2] = self.fc['2'](self.a[1])\n",
    "        self.a[2] = torch.relu(self.z[2])\n",
    "        \n",
    "        # Third layer \n",
    "        self.z[3] = self.fc['3'](self.a[2])\n",
    "        self.a[3] = torch.relu(self.z[3])\n",
    "        \n",
    "        # Fourth layer (output layer)\n",
    "        self.z[4] = self.fc['4'](self.a[3])\n",
    "        self.a[4] = self.z[4]\n",
    "        \n",
    "        return self.a[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152baec",
   "metadata": {},
   "source": [
    "## Define training pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94f24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, verbose = True):\n",
    "    # Neural network training pipeline using pytorch's SGD\n",
    "    \n",
    "    if verbose:\n",
    "        print(\" --------- Using Pytorch's SGD ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.double())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch % 5 == 0):\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da8daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, weight_decay, momentum, model, loss_fn, train_loader, verbose=True):\n",
    "    # Neural network training pipeline using manual gradient descent\n",
    "    if verbose:\n",
    "        print(\" --------- Using Manual Update ---------\")\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    b = [None, None, None, None, None, None, None, None]\n",
    "    \n",
    "    losses_train = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # For each batch in train_loader\n",
    "        for batch in train_loader:\n",
    "            # Split batch into  \n",
    "            inputs, labels = batch\n",
    "            \n",
    "            # Zero gradient for every batch\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs.double())\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()  \n",
    "            \n",
    "            # Adjust parameters\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                # Add L2 regularization\n",
    "                grad = p.grad + weight_decay*p.data\n",
    "                # Add momentum\n",
    "                if momentum != 0:\n",
    "                    if b[i] == None:\n",
    "                        b[i] = grad\n",
    "                    else:\n",
    "                        b[i] = momentum*b[i] + grad\n",
    "                    grad = b[i]\n",
    "                \n",
    "                p.data = p.data - lr*grad\n",
    "            \n",
    "            # Add loss from current batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses_train.append(epoch_loss / n_batches)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch % 5 == 0):\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "            \n",
    "    return losses_train, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2d098",
   "metadata": {},
   "source": [
    "Loop for hyperparameter testing and output print statements (made to match ``gradient_descent_output.txt``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08f1594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the train dataset:         9017\n",
      "Size of the validation dataset:    983\n",
      "Size of the test dataset:          2000\n",
      "\n",
      "Training on device cpu.\n",
      "\n",
      " ----- Global parameters ----- \n",
      "batch_size = 256\n",
      "n_epoch =  30\n",
      "loss_fn =  CrossEntropyLoss()\n",
      "seed =  123\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.000\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "10:27:56.702568  |  Epoch 1  |  Training loss 0.69640\n",
      "10:28:08.329524  |  Epoch 5  |  Training loss 0.55943\n",
      "10:28:22.791847  |  Epoch 10  |  Training loss 0.46984\n",
      "10:28:37.440455  |  Epoch 15  |  Training loss 0.42884\n",
      "10:28:51.849598  |  Epoch 20  |  Training loss 0.39155\n",
      "10:29:06.296613  |  Epoch 25  |  Training loss 0.35858\n",
      "10:29:20.657055  |  Epoch 30  |  Training loss 0.32692\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.8734\n",
      "Validation\n",
      "Accuracy: 0.8941\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "10:29:25.107618  |  Epoch 1  |  Training loss 0.68580\n",
      "10:29:37.060317  |  Epoch 5  |  Training loss 0.56936\n",
      "10:29:51.984957  |  Epoch 10  |  Training loss 0.47856\n",
      "10:30:06.993428  |  Epoch 15  |  Training loss 0.43433\n",
      "10:30:21.960439  |  Epoch 20  |  Training loss 0.39692\n",
      "10:30:36.892257  |  Epoch 25  |  Training loss 0.36451\n",
      "10:30:51.851727  |  Epoch 30  |  Training loss 0.33349\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.8702\n",
      "Validation\n",
      "Accuracy: 0.8941\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.00\n",
      "decay: 0.010\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "10:30:56.190147  |  Epoch 1  |  Training loss 0.67802\n",
      "10:31:07.905211  |  Epoch 5  |  Training loss 0.56513\n",
      "10:31:22.648223  |  Epoch 10  |  Training loss 0.48401\n",
      "10:31:37.356581  |  Epoch 15  |  Training loss 0.44285\n",
      "10:31:51.899353  |  Epoch 20  |  Training loss 0.40797\n",
      "10:32:06.557700  |  Epoch 25  |  Training loss 0.37835\n",
      "10:32:21.164997  |  Epoch 30  |  Training loss 0.35132\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.8595\n",
      "Validation\n",
      "Accuracy: 0.8941\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "10:32:25.603155  |  Epoch 1  |  Training loss 0.66961\n",
      "10:32:37.562617  |  Epoch 5  |  Training loss 0.54663\n",
      "10:32:52.878905  |  Epoch 10  |  Training loss 0.47725\n",
      "10:33:08.063488  |  Epoch 15  |  Training loss 0.43663\n",
      "10:33:22.943826  |  Epoch 20  |  Training loss 0.40228\n",
      "10:33:37.834758  |  Epoch 25  |  Training loss 0.37230\n",
      "10:33:52.797599  |  Epoch 30  |  Training loss 0.34457\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.8626\n",
      "Validation\n",
      "Accuracy: 0.8882\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.90\n",
      "decay: 0.000\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "10:33:57.149747  |  Epoch 1  |  Training loss 0.61383\n",
      "10:34:08.849164  |  Epoch 5  |  Training loss 0.36492\n",
      "10:34:23.477398  |  Epoch 10  |  Training loss 0.21153\n",
      "10:34:38.071384  |  Epoch 15  |  Training loss 0.14048\n",
      "10:34:52.680198  |  Epoch 20  |  Training loss 0.13526\n",
      "10:35:07.346785  |  Epoch 25  |  Training loss 0.16508\n",
      "10:35:21.964159  |  Epoch 30  |  Training loss 0.11156\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.9685\n",
      "Validation\n",
      "Accuracy: 0.9882\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "10:35:26.470894  |  Epoch 1  |  Training loss 0.61674\n",
      "10:35:39.112265  |  Epoch 5  |  Training loss 0.35812\n",
      "10:35:54.515804  |  Epoch 10  |  Training loss 0.22186\n",
      "10:36:10.081908  |  Epoch 15  |  Training loss 0.21689\n",
      "10:36:25.449205  |  Epoch 20  |  Training loss 0.08385\n",
      "10:36:40.852199  |  Epoch 25  |  Training loss 0.13077\n",
      "10:36:56.306921  |  Epoch 30  |  Training loss 0.09555\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.9584\n",
      "Validation\n",
      "Accuracy: 1.0000\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.90\n",
      "decay: 0.010\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "10:37:00.674909  |  Epoch 1  |  Training loss 0.60569\n",
      "10:37:12.552332  |  Epoch 5  |  Training loss 0.37070\n",
      "10:37:27.491866  |  Epoch 10  |  Training loss 0.26934\n",
      "10:37:42.260038  |  Epoch 15  |  Training loss 0.16947\n",
      "10:37:57.076183  |  Epoch 20  |  Training loss 0.14428\n",
      "10:38:11.880370  |  Epoch 25  |  Training loss 0.14119\n",
      "10:38:26.691943  |  Epoch 30  |  Training loss 0.09675\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.9382\n",
      "Validation\n",
      "Accuracy: 0.9588\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "10:38:31.164809  |  Epoch 1  |  Training loss 0.60922\n",
      "10:38:43.514227  |  Epoch 5  |  Training loss 0.37594\n",
      "10:38:58.892270  |  Epoch 10  |  Training loss 0.27157\n",
      "10:39:14.444783  |  Epoch 15  |  Training loss 0.20290\n",
      "10:39:30.031012  |  Epoch 20  |  Training loss 0.16241\n",
      "10:39:45.349860  |  Epoch 25  |  Training loss 0.14947\n",
      "10:40:00.700958  |  Epoch 30  |  Training loss 0.11725\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.9704\n",
      "Validation\n",
      "Accuracy: 0.9941\n",
      "\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.01\n",
      "mom: 0.80\n",
      "decay: 0.001\n",
      "\n",
      " --------- Using Pytorch's SGD ---------\n",
      "10:40:05.068209  |  Epoch 1  |  Training loss 0.62840\n",
      "10:40:16.906103  |  Epoch 5  |  Training loss 0.39623\n",
      "10:40:31.772005  |  Epoch 10  |  Training loss 0.29254\n",
      "10:40:46.545222  |  Epoch 15  |  Training loss 0.18813\n",
      "10:41:01.337389  |  Epoch 20  |  Training loss 0.14722\n",
      "10:41:16.217760  |  Epoch 25  |  Training loss 0.16027\n",
      "10:41:31.039054  |  Epoch 30  |  Training loss 0.07078\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.9067\n",
      "Validation\n",
      "Accuracy: 0.9353\n",
      "\n",
      " --------- Using Manual Update ---------\n",
      "10:41:35.482055  |  Epoch 1  |  Training loss 0.64353\n",
      "10:41:47.731972  |  Epoch 5  |  Training loss 0.40741\n",
      "10:42:03.110997  |  Epoch 10  |  Training loss 0.30360\n",
      "10:42:18.498975  |  Epoch 15  |  Training loss 0.20044\n",
      "10:42:33.825355  |  Epoch 20  |  Training loss 0.14292\n",
      "10:42:49.250626  |  Epoch 25  |  Training loss 0.18007\n",
      "10:43:04.583639  |  Epoch 30  |  Training loss 0.07306\n",
      "\n",
      " --- Accuracies ---\n",
      "Training\n",
      "Accuracy: 0.8750\n",
      "Validation\n",
      "Accuracy: 0.8941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(y_pred, y_true):\n",
    "    #TODO: vary good (too good?) validation accuracies seem unlikely? Is there a mistake here or somewhere else?\n",
    "    \"\"\" calculates fraction of predictions that are correct \"\"\"\n",
    "    good_predictions = (y_pred == y_true)\n",
    "    # counting number of true entries in boolean tensor\n",
    "    correct = len(good_predictions.masked_select(good_predictions == True))\n",
    "    acc = correct / len(y_true) # fraction of correct predictions\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# set global hyperparameters\n",
    "n_epochs = 30\n",
    "batch_size = 256\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.01\n",
    "\n",
    "# load data\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Split X and y\n",
    "train_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_train), shuffle=False)\n",
    "for data in train_set:\n",
    "    x_train, y_train = data\n",
    "val_set = torch.utils.data.DataLoader(data_train, batch_size=len(data_val), shuffle=False)\n",
    "for data in val_set:\n",
    "    x_validation, y_validation = data\n",
    "\n",
    "print('\\nTraining on device cpu.\\n')\n",
    "\n",
    "print(' ----- Global parameters ----- ')\n",
    "print('batch_size = %d'%batch_size)\n",
    "print('n_epoch =  %d'%n_epochs)\n",
    "print('loss_fn =  CrossEntropyLoss()')\n",
    "print('seed =  %d'%seed)\n",
    "# for storing which MLP does the best\n",
    "current_acc = 0\n",
    "val_accs = []\n",
    "\n",
    "# parameters to test (same as from output text)\n",
    "moms = [0, 0, 0.9, 0.9, 0.8]\n",
    "decays = [0, 0.01, 0, 0.01, 0.001, 0.01]\n",
    "\n",
    "\n",
    "# use list of models to avoid having to train them so much? \n",
    "# not sure how this will work with all the pytorch subclass stuff\n",
    "\n",
    "for i in range(len(moms)):\n",
    "    mom = moms[i]\n",
    "    decay = decays[i]\n",
    "    \n",
    "    print('\\n =========================================================\\n  Current parameters:')\n",
    "    print('lr: %.2f'%lr)\n",
    "    print('mom: %.2f'%mom)\n",
    "    print('decay: %.3f\\n'%decay)\n",
    "    \n",
    "    # Train your Pytorch's SGD MLP (model1) here and return y_pred_train from last loop of training\n",
    "    model1 = MyMLP()\n",
    "    optimizer = torch.optim.SGD(model1.parameters(), lr=lr, weight_decay=decay, momentum=mom)\n",
    "    losses_train1, model1 = train(n_epochs, optimizer, model1, loss_fn, train_loader)\n",
    "    y_pred_train = model1(x_train.double())\n",
    "    y_pred_train = y_pred_train.max(axis=1)[1]\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model1(x_validation.double())\n",
    "    y_pred_val = y_pred_val.max(axis=1)[1] # find which label is most likely (0-bird, 1-plane) \n",
    "    # TODO: check above statement\n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'PyTorch SDG'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)\n",
    "    \n",
    "    # Train your manual update MLP (model2) here and return y_pred_train from last loop of training\n",
    "    model2 = MyMLP()\n",
    "    losses_train2, model2 = train_manual_update(n_epochs, lr, decay, mom, model2, loss_fn, train_loader)\n",
    "    y_pred_train = model2(x_train.double())\n",
    "    y_pred_train = y_pred_train.max(axis=1)[1]\n",
    "    \n",
    "    # calculating accuracies from training\n",
    "    acc_t = calculate_accuracy(y_pred_train, y_train)\n",
    "    \n",
    "    # calculate accuracies from validation set\n",
    "    y_pred_val = model2(x_validation.double())\n",
    "    y_pred_val = y_pred_val.max(axis=1)[1]\n",
    "    acc_v = calculate_accuracy(y_pred_val, y_validation)\n",
    "    val_accs.append(acc_v)\n",
    "    if acc_v >= current_acc:\n",
    "        best_mom = mom\n",
    "        best_decay = decay\n",
    "        best_SDG = 'Manual update'\n",
    "        current_acc = acc_v\n",
    "    \n",
    "    print('\\n --- Accuracies ---')\n",
    "    print('Training\\nAccuracy: %.2f'%acc_t)\n",
    "    print('Validation\\nAccuracy: %.2f\\n'%acc_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c25e7",
   "metadata": {},
   "source": [
    "Questions 9 and 10:\n",
    "Select the best model among those trained in the previous question based on their accuracy. Evaluate the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6230874d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Best model ---\n",
      "Learning rate: 0.01\n",
      "Momentum: 0.90\n",
      "Decay: 0.000\n",
      "SDG method: Manual update\n",
      "Validation accuracy: 1.000\n",
      "\n",
      "Test accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "print('\\n --- Best model ---')\n",
    "print('Learning rate: %.2f'%lr)\n",
    "print('Momentum: %.2f'%best_mom)\n",
    "print('Decay: %.3f'%best_decay)\n",
    "print('SDG method: ' + best_SDG)\n",
    "print('Validation accuracy: %.3f\\n'%current_acc)\n",
    "\n",
    "\n",
    "test_set = torch.utils.data.DataLoader(data_test, batch_size=len(data_test), shuffle=False)\n",
    "for data in test_set:\n",
    "    x_test, y_test = data\n",
    "    \n",
    "model = MyMLP()\n",
    "if best_SDG == 'PyTorch SDG':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=best_decay, momentum=best_mom)\n",
    "    losses_train, model = train(n_epochs, optimizer, model, loss_fn, train_loader, verbose=False)\n",
    "else:\n",
    "    losses_train, model = train_manual_update(n_epochs, lr, best_decay, best_mom, model, loss_fn, train_loader, verbose=False)\n",
    "y_pred_test = model(x_test.double())\n",
    "y_pred_test = y_pred_test.max(axis=1)[1]\n",
    "acc_test = calculate_accuracy(y_pred_test, y_test)\n",
    "\n",
    "print('Test accuracy: %.2f'%acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5202eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = [i for i in range(n_epochs)]\n",
    "plt.plot(epochs, losses_train)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
