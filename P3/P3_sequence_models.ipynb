{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed860bb",
   "metadata": {},
   "source": [
    "# Project 3 - Sequence models\n",
    "\n",
    "See the Project 3 text for more information about what is done in this project. Also see PDF supplementing this assignment.\n",
    "\n",
    "(Note that we use code similar to/copied from the weekly tasks and earlier projects.)\n",
    "\n",
    "Import relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff813905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# We use torch.double to get the same results as PyTorch\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# run the training on CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# local path to data\n",
    "path = '../../project3/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8d2b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a36c60af",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1.1 Reading and tokenizing the data sets\n",
    "\n",
    "We define a function for reading the txt files line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec00468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(datapath='./'):\n",
    "    \"\"\"\n",
    "    Return a list of strings, one for each line of text in each .txt files in 'datapath'\n",
    "    \"\"\"\n",
    "    # Find all txt files in directory \n",
    "    files = listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith(\".txt\")]\n",
    "    \n",
    "    # Stores each line of each book in a list\n",
    "    lines = []\n",
    "    for f_name in files:\n",
    "        lines += open(f_name, encoding='utf8').readlines()\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3416e",
   "metadata": {},
   "source": [
    "We can then read the files from our local path using the eight books from the training, validation, and test data folders in the pre-made datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa8d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_train = read_files(path + 'data_train/')\n",
    "lines_val = read_files(path + 'data_val/')\n",
    "lines_test = read_files(path + 'data_test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9fb6b",
   "metadata": {},
   "source": [
    "We define a function used to tokenize the sequences using torchtext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc7c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer will split a long text into a list of english words\n",
    "tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "\n",
    "def tokenize(lines):\n",
    "    \"\"\"\n",
    "    Tokenize the list of lines\n",
    "    \"\"\"\n",
    "    \n",
    "    list_text = []\n",
    "    for line in lines:\n",
    "        list_text += tokenizer(line)\n",
    "        \n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff3582",
   "metadata": {},
   "source": [
    "And tokenize the data in the three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f816b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:    1368807\n",
      "Total number of words in the validation dataset:  49526\n",
      "Total number of words in the test dataset:        131750\n"
     ]
    }
   ],
   "source": [
    "words_train = tokenize(lines_train)\n",
    "words_val = tokenize(lines_val)\n",
    "words_test = tokenize(lines_test)\n",
    "\n",
    "print(\"Total number of words in the training dataset:   \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset: \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:       \", len(words_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd49e6f",
   "metadata": {},
   "source": [
    "### 2.1.2 Define vocabulary\n",
    "\n",
    "We define functions used to build the vocabulary and to count the occurence frequency in a dataset of the words in the vocabulary. Note that we ignore names and digits when building the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44e5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match any word containing digit\n",
    "no_digits = '\\w*[0-9]+\\w*'\n",
    "# Match word containing an uppercase \n",
    "no_names = '\\w*[A-Z]+\\w*'\n",
    "# Match any sequence containing more than one space\n",
    "no_spaces = '\\s+'\n",
    "\n",
    "def yield_tokens(lines):\n",
    "    \"\"\"\n",
    "    Yield tokens, ignoring names and digits to build vocabulary\n",
    "    \"\"\"\n",
    "    for line in lines:\n",
    "        line = re.sub(no_digits + \"|\" + no_names, ' ', line)\n",
    "        line = re.sub(no_spaces, ' ', line)\n",
    "        yield tokenizer(line)\n",
    "        \n",
    "        \n",
    "def count_freqs(data, vocab):\n",
    "    \"\"\"\n",
    "    Count occurrences of each word in vocabulary in the data\n",
    "    \"\"\"\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in data:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc4d71",
   "metadata": {},
   "source": [
    "We can now build the vocabulary of the words used a minimum of 100 times in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7b4cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:     1368807\n",
      "Number of distinct words in the training dataset:  30374\n",
      "Size of defined vocabulary:                        1050\n"
     ]
    }
   ],
   "source": [
    "# vocab contains the vocabulary found in the data, associating an index to each word\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(lines_train), min_freq = 100, specials = [\"<unk>\"])\n",
    "# Since we removed all words with an uppercase when building the vocabulary, we skipped the word \"I\"\n",
    "vocab.append_token(\"i\")\n",
    "\n",
    "# Value of default index. This index will be returned when OOV (Out Of Vocabulary) token is queried.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total number of words in the training dataset:    \", len(words_train))\n",
    "print(\"Number of distinct words in the training dataset: \", len(set(words_train)))\n",
    "print(\"Size of defined vocabulary:                       \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a5605",
   "metadata": {},
   "source": [
    "We can also take a look at the frequency of the words in the vocabulary in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe6707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occurences:\n",
      " [(251055, '<unk>'), (89904, ','), (71106, 'the'), (63121, '.'), (43426, 'and'), (33952, 'to'), (30061, 'of'), (23575, 'a'), (18657, 'in'), (20755, 'he'), (16814, 'that'), (15056, 'was'), (14400, 'his'), (13815, 'it'), (10997, 'with'), (10735, 'had'), (9430, 'her'), (9334, 'not'), (10562, 'you'), (9198, 'as'), (9152, 'at'), (8447, 'him'), (8457, 'is'), (8269, 'for'), (7824, 'on'), (7122, '!'), (6510, '?'), (8087, 'she'), (6477, 's'), (6223, 'be'), (5871, 'said'), (8103, 'but'), (6221, 'all'), (5689, 'have'), (5137, 'from'), (4752, 'which'), (4648, 'me'), (5138, 'so'), (4722, 'by'), (4476, 'were'), (4807, 'my'), (4989, 'this'), (4952, 'they'), (4399, 'one'), (4023, 'who'), (4656, 'what'), (3253, 'up'), (4196, 'there'), (3211, 'them'), (4125, 'we'), (3206, 'would'), (3258, 'an'), (3196, 'are'), (3091, 'been'), (3009, 'or'), (2941, 'out'), (2985, 'will'), (3912, 'when'), (3587, 'no'), (2656, 'could'), (2631, 'man'), (3321, 'if'), (2606, 'did'), (2580, 'their'), (2504, 'into'), (2723, 'do'), (2239, 'more'), (2385, 'your'), (2603, 'now'), (2286, 'only'), (2180, 'very'), (2032, 'time'), (2013, 'about'), (3231, 'then'), (2031, 'some'), (1879, 'went'), (1830, 'came'), (1894, 'see'), (1824, 'down'), (1841, 'has'), (1779, 'know'), (1751, 'himself'), (1738, 't'), (1757, 'little'), (1743, 'before'), (1713, 'can'), (1668, 'again'), (1598, 'like'), (1566, 'must'), (1546, 'over'), (1511, 'eyes'), (1493, 'us'), (1641, 'go'), (1458, 'other'), (1460, 'am'), (1542, 'our'), (1470, 'old'), (1434, 'away'), (1413, 'face'), (1938, 'how'), (1416, 'should'), (1610, 'come'), (1457, 'two'), (1344, 'room'), (1337, 'thought'), (1305, 'than'), (1298, 'back'), (1342, 'without'), (1276, 'any'), (1316, 'such'), (1448, 'where'), (1589, 'after'), (1234, 'made'), (1217, 'way'), (1230, 'shall'), (1235, 'men'), (1304, 'good'), (1177, 'say'), (1180, 'its'), (1154, 'looked'), (1169, 'day'), (1214, 'still'), (1145, 'hand'), (1193, 'first'), (1130, 'saw'), (1129, 'same'), (1114, 'asked'), (1112, 'seemed'), (1117, 'took'), (1110, 'own'), (1195, 'may'), (1089, 'head'), (1092, 'left'), (1075, 'began'), (1080, 'long'), (1076, 'upon'), (1166, 'nothing'), (1143, 'even'), (1257, 'just'), (1091, 'life'), (1056, 'off'), (1127, 'another'), (1030, 'felt'), (1031, 'much'), (1021, 'door'), (1219, 'here'), (1042, 'those'), (1066, 'never'), (1003, 'something'), (1073, 'these'), (985, 'think'), (963, 'too'), (949, 'heard'), (1608, 'well'), (1003, 'once'), (952, 'through'), (945, 'last'), (921, 'being'), (946, 'great'), (912, 'house'), (973, 'take'), (953, 'look'), (846, 'right'), (840, 'might'), (841, 'young'), (863, 'get'), (834, 'night'), (827, 'whole'), (833, 'round'), (821, 'found'), (837, 'quite'), (816, 'make'), (814, 'things'), (790, 'told'), (857, 'tell'), (769, 'knew'), (782, 'always'), (764, 'moment'), (771, 'people'), (841, 'though'), (804, 'dear'), (816, 'yet'), (988, 'let'), (740, 'put'), (710, '('), (710, ')'), (773, 'father'), (799, 'while'), (772, 'every'), (697, 'turned'), (695, 'done'), (686, 'gave'), (693, 'wife'), (816, 'having'), (695, 'love'), (671, 'place'), (701, 'give'), (706, 'because'), (662, 'going'), (655, 'got'), (663, 'whom'), (663, 'woman'), (659, 'army'), (652, 'morning'), (663, 'under'), (639, 'sat'), (663, 'most'), (636, 'seen'), (681, 'three'), (619, 'herself'), (619, 'also'), (630, 'friend'), (607, 'voice'), (608, 'anything'), (623, 'looking'), (603, 'side'), (638, 'soon'), (760, 'don'), (621, 'already'), (595, 'find'), (580, 'stood'), (599, 'mind'), (578, 'words'), (609, 'behind'), (649, 'everything'), (573, 'thing'), (558, 'hands'), (550, 'll'), (541, 'cried'), (543, 'understand'), (543, 'against'), (538, 'answered'), (558, 'many'), (552, 'others'), (536, 'home'), (530, 'alone'), (528, 'part'), (526, 'few'), (525, 'matter'), (523, 'ran'), (537, 'end'), (512, 'want'), (514, 'set'), (605, 'suddenly'), (562, 'mother'), (502, 'brought'), (503, 'heart'), (551, 'each'), (941, 'why'), (539, 'both'), (500, 'ever'), (487, 'myself'), (490, 'along'), (485, 'smile'), (562, 'general'), (499, 'between'), (480, 'taken'), (474, 'work'), (463, 'replied'), (959, 'count'), (463, 'passed'), (462, 'cannot'), (466, 'son'), (507, 'new'), (458, 'white'), (464, 'husband'), (459, 'far'), (453, 'small'), (458, 'together'), (452, 'feeling'), (452, 'officer'), (452, 'became'), (450, 'called'), (465, 'death'), (449, 'word'), (456, 'open'), (454, 'till'), (441, 'light'), (441, 'lay'), (459, 'leave'), (1025, 'princess'), (438, 'better'), (434, 'gone'), (426, 'table'), (425, 'window'), (446, 'does'), (425, 'evening'), (417, 'bed'), (2003, 'prince'), (415, 'world'), (414, 'continued'), (412, 'days'), (413, 'full'), (448, 'poor'), (434, 'order'), (404, 'front'), (402, 'remained'), (403, 'case'), (421, 'horse'), (408, 'ask'), (407, 'coming'), (396, 'fell'), (425, 'evidently'), (401, 'fire'), (390, 'sent'), (432, 'since'), (392, 'near'), (399, 'nor'), (377, 'themselves'), (378, 'saying'), (387, 'speak'), (495, 'king'), (402, 'whether'), (461, 'however'), (370, 'longer'), (377, 'help'), (375, 'power'), (368, 'battle'), (493, 'countess'), (367, 'given'), (377, 'soldiers'), (367, 'train'), (369, 'fear'), (364, 'position'), (361, 'spoke'), (361, 'sleep'), (360, 'wish'), (356, 'ready'), (353, 'fellow'), (353, 'wanted'), (352, 'entered'), (350, 'horses'), (351, 'yourself'), (366, 'letter'), (346, 'possible'), (417, 'next'), (343, 'opened'), (346, 'sitting'), (341, 'expression'), (340, 'soul'), (340, 'hair'), (339, 'years'), (336, 'question'), (367, 'really'), (349, 'strange'), (336, 'air'), (367, 'among'), (336, 'hear'), (333, 'held'), (333, 'lost'), (336, 'money'), (337, 'talk'), (337, 'dead'), (332, 'met'), (335, 'happened'), (335, 'almost'), (325, 'course'), (326, 'rest'), (324, 'able'), (330, 'happy'), (322, 'returned'), (340, 'road'), (346, 'black'), (320, 'child'), (318, 'girl'), (324, 'rose'), (323, 'taking'), (315, 'wished'), (317, 'large'), (330, 'half'), (359, 'lady'), (310, 'best'), (309, 'stopped'), (326, 'keep'), (321, 'true'), (308, 'present'), (316, 'terrible'), (306, 'itself'), (308, 'past'), (339, 'war'), (325, 'believe'), (324, 'beside'), (305, 'sound'), (303, 'received'), (307, 'blood'), (301, 'body'), (302, 'enemy'), (301, 'tried'), (308, 'enough'), (310, 'red'), (300, 'kind'), (299, 'moved'), (303, 'forward'), (298, 'daughter'), (300, 'feel'), (304, 'standing'), (299, 'beautiful'), (299, 'reason'), (298, 'times'), (297, 'arms'), (294, 'clock'), (293, 'business'), (300, 'commander'), (295, 'free'), (295, 'dark'), (292, 'followed'), (290, 'feet'), (290, 'action'), (289, 'else'), (292, 'country'), (286, 'added'), (286, 'become'), (293, 'chief'), (286, 'less'), (286, 'line'), (293, 'read'), (302, 'water'), (285, 'cause'), (284, 'hour'), (284, 'close'), (289, 'rather'), (290, 'within'), (278, 'kept'), (275, 'need'), (298, 'toward'), (299, 'seeing'), (274, 'shouted'), (288, 'until'), (273, 'cold'), (272, 'doing'), (276, 'name'), (270, 'fact'), (275, 'run'), (271, 'troops'), (269, 'crowd'), (282, 'either'), (280, 'turning'), (269, 'ground'), (273, 'afraid'), (270, 'answer'), (272, 'certain'), (267, 'lips'), (265, 'arm'), (265, 'anyone'), (263, 'doubt'), (341, 'during'), (273, 'whose'), (270, 'cut'), (274, 'second'), (264, 'live'), (261, 'silent'), (262, 'soldier'), (260, 'drew'), (263, 'sure'), (268, 'turn'), (354, 'o'), (257, 'rode'), (257, 'understood'), (256, 'idea'), (254, 'appeared'), (256, 'children'), (295, 'doctor'), (254, 'noticed'), (253, 'across'), (263, 'around'), (298, 'sir'), (254, 'talking'), (250, 'thousand'), (249, 'waiting'), (251, 'dinner'), (249, 'clear'), (249, 'return'), (252, 'silence'), (256, 'above'), (254, 'impossible'), (245, 'minutes'), (263, 'bring'), (257, 'women'), (244, 'officers'), (244, 'making'), (352, 'perhaps'), (246, 'late'), (246, 'short'), (243, 'drawing'), (243, 'news'), (248, 'tears'), (239, 'used'), (241, 'boy'), (242, 'pleasure'), (238, 'reached'), (242, 'bad'), (237, 'grew'), (237, 'movement'), (327, 'nature'), (236, 'carried'), (258, 'four'), (236, 'quickly'), (965, 'yes'), (235, 'care'), (277, 'later'), (237, 'says'), (234, 'sight'), (234, 'wounded'), (237, 'high'), (272, 'everyone'), (232, 'struck'), (231, 'led'), (271, 'master'), (231, 'necessary'), (233, 'hope'), (234, 'use'), (275, 'remember'), (243, 'brother'), (276, 'several'), (228, 've'), (226, 'manner'), (228, 'seems'), (227, 'thinking'), (227, 'carriage'), (224, 'sort'), (230, 'killed'), (224, 'strength'), (222, 'hundred'), (230, 'orders'), (225, 'regiment'), (235, 'state'), (222, 'known'), (224, 'point'), (229, 'hard'), (220, 'laid'), (227, 'often'), (225, 'speaking'), (227, 'third'), (7, '-'), (219, 'least'), (219, 'usual'), (221, 'pale'), (217, 'different'), (238, 'fine'), (217, 'smiled'), (228, 'call'), (215, 'closed'), (217, 'town'), (215, 'corner'), (215, 'steps'), (226, 'won'), (214, 'asleep'), (213, 'seem'), (212, 'family'), (212, 'meet'), (213, 'conversation'), (257, 'indeed'), (212, 'arrived'), (227, 'immediately'), (211, 'raised'), (210, 'loved'), (211, 'truth'), (231, 'five'), (213, 'glad'), (210, 'heavy'), (209, 'attention'), (236, 'neither'), (206, 'repeated'), (204, 'sun'), (204, 'threw'), (204, 'change'), (203, 'listened'), (206, 'trying'), (202, 'remarked'), (202, 'snow'), (388, 'm'), (203, 'show'), (197, 'duty'), (196, 'showed'), (195, 'tone'), (194, 'force'), (195, 'smiling'), (197, 'straight'), (208, 'ten'), (214, 'pass'), (197, 'service'), (216, 'company'), (194, 'strong'), (200, 'try'), (192, 'village'), (191, 'merely'), (192, 'presence'), (199, 'towards'), (190, 'caught'), (213, 'd'), (190, 'mouth'), (193, 'study'), (188, 'deep'), (188, 'die'), (191, 'eat'), (192, 'gold'), (202, 'hardly'), (188, 'low'), (192, 'story'), (186, 'placed'), (187, 'reply'), (186, 'walked'), (186, 'desire'), (194, 'kill'), (233, 'wait'), (188, 'wood'), (190, 'de'), (183, 'latter'), (183, 'mean'), (182, 'exclaimed'), (182, 'frightened'), (182, 'glanced'), (182, 'ordered'), (189, 'sister'), (182, 'thin'), (184, 'fall'), (181, 'former'), (180, 'instant'), (183, 'remain'), (180, 'watch'), (180, 'angry'), (179, 'cry'), (180, 'faces'), (182, 'running'), (181, 'smoke'), (179, 'trouble'), (192, 'someone'), (180, 'big'), (177, 'happiness'), (178, 'human'), (177, 'lived'), (178, 'lying'), (177, 'neck'), (177, 'shoulders'), (218, 'certainly'), (177, 'earth'), (178, 'holding'), (175, 'comes'), (175, 're'), (175, 'fresh'), (184, 'hold'), (175, 'surprise'), (177, 'thoughts'), (176, 'events'), (174, 'leaving'), (172, 'coat'), (173, 'knows'), (173, 'pretty'), (173, 'voices'), (171, 'chair'), (171, 'glass'), (172, 'important'), (170, 'fixed'), (171, 'simply'), (194, 'therefore'), (182, 'history'), (169, 'interest'), (206, 'sometimes'), (170, 'married'), (174, 'station'), (166, 'flew'), (168, 'getting'), (166, 'opinion'), (206, 'please'), (166, 'act'), (167, 'dress'), (165, 'drove'), (164, 'account'), (164, 'ago'), (180, 'beyond'), (164, 'hours'), (166, 'living'), (165, 'move'), (169, 'bear'), (167, 'contrary'), (166, 'moving'), (164, 'questions'), (164, 'year'), (162, 'broken'), (162, 'floor'), (162, 'kissed'), (165, 'ought'), (166, 'peace'), (161, 'effort'), (161, 'forest'), (168, 'passing'), (162, 'pleased'), (162, 'quiet'), (162, 'unable'), (162, 'affairs'), (172, 'field'), (161, 'giving'), (160, 'subject'), (164, 'adjutant'), (159, 'laughed'), (160, 'legs'), (158, 'sides'), (160, 'wolf'), (160, 'foot'), (158, 'friends'), (157, 'papers'), (157, 'started'), (160, 'blue'), (158, 'danger'), (159, 'joy'), (157, 'seized'), (156, 'beginning'), (155, 'box'), (156, 'command'), (158, 'especially'), (169, 'outside'), (154, 'handsome'), (177, 'none'), (154, 'carry'), (157, 'bird'), (157, 'following'), (154, 'further'), (157, 'honor'), (154, 'step'), (152, 'view'), (166, 'castle'), (150, 'common'), (151, 'letters'), (150, 'shook'), (151, 'wind'), (150, 'expected'), (160, 'six'), (165, 'society'), (158, 'bridge'), (150, 'evil'), (148, 'remembered'), (148, 'sad'), (154, 'staff'), (147, 'means'), (151, 'police'), (148, 'rushed'), (147, 'asking'), (146, 'ball'), (151, 'calm'), (146, 'changed'), (149, 'pain'), (146, 'wrong'), (145, 'eye'), (145, 'sake'), (152, 'stay'), (144, 'surprised'), (151, 'chance'), (143, 'distance'), (147, 'freedom'), (150, 'stone'), (143, 'matters'), (142, 'person'), (142, 'purpose'), (159, 'sit'), (155, 'write'), (142, 'bright'), (143, 'gentleman'), (142, 'died'), (140, 'driver'), (143, 'figure'), (140, 'glance'), (150, 'quick'), (143, 'real'), (140, 'single'), (140, 'spot'), (142, 'follow'), (139, 'occurred'), (145, 'save'), (140, 'fallen'), (138, 'ladies'), (151, 'send'), (153, 'seven'), (143, 'suppose'), (139, 'drawn'), (138, 'finished'), (147, 'law'), (139, 'particularly'), (141, 'walk'), (137, 'wrote'), (136, 'happen'), (142, 'knowing'), (137, 'paper'), (137, 'pity'), (140, 'tree'), (135, 'affair'), (135, 'covered'), (135, 'direction'), (136, 'express'), (136, 'occupied'), (192, 'thus'), (136, 'begin'), (136, 'journey'), (134, 'simple'), (136, 'written'), (133, 'broke'), (133, 'decided'), (138, 'garden'), (135, 'touched'), (132, 'considered'), (158, 'court'), (134, 'dog'), (132, 'experienced'), (132, 'formed'), (133, 'plan'), (132, 'spite'), (140, 'afterwards'), (132, 'laughing'), (134, 'marry'), (138, 'number'), (132, 'pressed'), (196, 'captain'), (132, 'dressed'), (130, 'expressed'), (130, 'meant'), (132, 'peasant'), (167, 'whatever'), (130, 'whispered'), (129, 'bent'), (131, 'forget'), (129, 'hat'), (129, 'interrupted'), (130, 'peasants'), (129, 'fast'), (131, 'looks'), (131, 'middle'), (128, 'minute'), (128, 'spent'), (128, 'wall'), (128, 'wild'), (133, 'clearly'), (128, 'easy'), (128, 'ill'), (127, 'knife'), (131, 'laugh'), (127, 'piece'), (127, 'sky'), (127, 'attack'), (130, 'excellency'), (126, 'meaning'), (127, 'pleasant'), (126, 'sense'), (128, 'drink'), (128, 'except'), (130, 'imagine'), (132, 'according'), (124, 'caused'), (124, 'ceased'), (124, 'effect'), (128, 'post'), (124, 'prepared'), (126, 'seat'), (127, 'tall'), (142, 'early'), (123, 'engine'), (124, 'knowledge'), (128, 'meeting'), (125, 'rapidly'), (123, 'rooms'), (132, 'search'), (125, 'visit'), (123, 'waited'), (123, 'clothes'), (123, 'difficult'), (123, 'notice'), (123, 'prisoners'), (122, 'quietly'), (122, 'result'), (145, 'stop'), (123, 'telling'), (133, 'to-night'), (122, 'darkness'), (141, 'everybody'), (132, 'golden'), (122, 'maid'), (121, 'serious'), (121, 'burning'), (122, 'farther'), (120, 'finger'), (121, 'form'), (125, 'hall'), (120, 'inquired'), (120, 'sprang'), (120, 'crime'), (119, 'shoulder'), (119, 'throat'), (119, 'enter'), (123, 'gate'), (118, 'teeth'), (119, 'unknown'), (118, 'week'), (117, 'activity'), (124, 'blow'), (118, 'deal'), (118, 'dying'), (117, 'empty'), (118, 'historians'), (117, 'pointed'), (117, 'talked'), (117, 'anxious'), (116, 'ears'), (116, 'gazed'), (124, 'land'), (122, 'lie'), (116, 'play'), (121, 'twenty'), (117, 'wine'), (115, 'ourselves'), (115, 'path'), (130, 'sea'), (116, 'settled'), (116, 'spoken'), (114, 'aim'), (114, 'future'), (114, 'greater'), (140, 'listen'), (114, 'marriage'), (114, 'paused'), (114, 'shouting'), (116, 'stand'), (113, 'aside'), (113, 'beauty'), (113, 'fingers'), (114, 'learned'), (125, 'military'), (116, 'mine'), (121, 'note'), (113, 'particular'), (115, 'ship'), (206, 'street'), (142, 'whilst'), (116, 'youth'), (112, 'alive'), (112, 'approached'), (113, 'campaign'), (112, 'cap'), (112, 'disappeared'), (114, 'sign'), (114, 'sounds'), (112, 'lit'), (119, 'river'), (116, 'shut'), (112, 'sweet'), (111, 'thrown'), (111, 'watched'), (131, 'city'), (110, 'grown'), (112, 'regard'), (111, 'understanding'), (112, 'various'), (109, 'condition'), (145, 'consider'), (109, 'makes'), (112, 'satisfied'), (111, 'slowly'), (110, 'start'), (126, 'tomorrow'), (109, 'windows'), (118, 'below'), (108, 'evident'), (108, 'fight'), (110, 'horror'), (121, 'reading'), (113, 'ring'), (109, 'servants'), (108, 'wide'), (107, 'bit'), (140, 'colonel'), (107, 'event'), (115, 'fate'), (109, 'filled'), (107, 'habit'), (112, 'hearing'), (126, 'instead'), (107, 'lifted'), (110, 'murder'), (109, 'sorry'), (108, 'special'), (109, 'warm'), (106, 'fit'), (106, 'handed'), (109, 'health'), (106, 'passage'), (106, 'pocket'), (108, 'shot'), (106, 'shown'), (107, 'suffering'), (107, 'terror'), (105, 'begun'), (109, 'drive'), (107, 'experience'), (107, 'key'), (106, 'lies'), (105, 'party'), (108, 'secret'), (104, 'advanced'), (104, 'burst'), (104, 'facts'), (105, 'girls'), (107, 'learn'), (108, 'nearer'), (105, 'report'), (104, 'sofa'), (103, 'appear'), (103, 'appearance'), (104, 'breath'), (105, 'continually'), (103, 'difficulty'), (103, 'dream'), (103, 'explain'), (104, 'food'), (103, 'galloped'), (103, 'gesture'), (104, 'growing'), (103, 'length'), (103, 'miles'), (103, 'passion'), (103, 'places'), (103, 'slept'), (106, 'spirit'), (121, 'twice'), (103, 'carefully'), (116, 'clever'), (104, 'escape'), (102, 'forehead'), (102, 'guns'), (105, 'hussars'), (107, 'nearly'), (102, 'presented'), (103, 'putting'), (102, 'stepped'), (102, 'uniform'), (104, 'worse'), (103, 'anger'), (117, 'hill'), (101, 'liked'), (105, 'listening'), (103, 'tea'), (100, 'age'), (100, 'bound'), (100, 'character'), (100, 'couple'), (100, 'crossed'), (102, 'generals'), (112, 'kindly'), (100, 'natural'), (111, 'probably'), (100, 'pushed'), (100, 'safe'), (103, 'supper'), (16404, 'i')]\n"
     ]
    }
   ],
   "source": [
    "freqs = count_freqs(words_train, vocab)\n",
    "print(\"occurences:\\n\", [(f.item(), w) for (f, w)  in zip(freqs, vocab.lookup_tokens(range(vocab_size)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43dbfa",
   "metadata": {},
   "source": [
    "We lookup the indices of the special symbols in the vocabulary for use when creating datasets later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c5dc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = vocab.lookup_indices(['<unk>',',','.','!','?','(',')','-'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeddfa5",
   "metadata": {},
   "source": [
    "### 2.1.3 n-gram language model architecture\n",
    "\n",
    "We define a n-gram model with an embedding layer. We choose to use two words before and two words after the target as context words, four in total. We also have a training function for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb962aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,         # size of vocabulary\n",
    "                 embedding_dim = 16, # small embedding dimension to reduce computations\n",
    "                 context_size = 4):  # n=4, two words before and two after\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Model architecture with embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = torch.flatten(embeds, 1)\n",
    "        y = torch.relu(self.fc1(embeds))\n",
    "        y = self.fc2(y)\n",
    "        y = nn.functional.log_softmax(y, dim=1)\n",
    "        return y\n",
    "    \n",
    "def train(model,        # n-gram model to be trained\n",
    "          n_epochs,     # number of epochs to train for\n",
    "          train_loader, # dataloader\n",
    "          optimizer,    # optimizer\n",
    "          loss_fn):     # loss function\n",
    "    \"\"\"\n",
    "    Training an n-gram model architecture and return trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for contexts, targets in train_loader:\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            output = model(contexts)\n",
    "            \n",
    "            loss = loss_fn(output, targets.flatten())\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f6f56",
   "metadata": {},
   "source": [
    "#### Making context/target datasets\n",
    "\n",
    "We need to make context/target datasets for use in the n-gram model, and define a function for making these datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe1f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text,               # all words in the dataset\n",
    "                    vocab,             # vocabulary\n",
    "                    context_size = 4): # effectively two words before and two words after target\n",
    "    \"\"\"\n",
    "    Create context/target dataset with integer features\n",
    "    \"\"\"\n",
    "    # two words before and two words after\n",
    "    context_size = context_size//2\n",
    "    \n",
    "    # number of words in text\n",
    "    n_text = len(text)\n",
    "    \n",
    "    # Transform the text to a list of integers\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size, n_text - context_size):\n",
    "        if txt[i] in specials: # see above - removing ['<unk>',',','.','!','?','(',')','-'] from datasets\n",
    "            continue\n",
    "        # Get context words before and after target        \n",
    "        contexts.append(torch.LongTensor([txt[i-context_size+j] for j in range(context_size*2+1) if j!=context_size]))\n",
    "        targets.append(txt[i])\n",
    "        \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.LongTensor(targets)\n",
    "    targets = torch.unsqueeze(targets, dim = 1)\n",
    "    data = torch.utils.data.TensorDataset(contexts, targets)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb9dd2",
   "metadata": {},
   "source": [
    "We create the context/target datasets (training, validation, and test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a3885f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the training dataset:    949667\n",
      "Total number of entries in the validation dataset:  34301\n",
      "Total number of entries in the test dataset:        86268\n"
     ]
    }
   ],
   "source": [
    "data_train = create_dataset(words_train, vocab)\n",
    "data_val = create_dataset(words_val, vocab)\n",
    "data_test = create_dataset(words_test, vocab)\n",
    "\n",
    "print(\"Total number of entries in the training dataset:   \", len(data_train))\n",
    "print(\"Total number of entries in the validation dataset: \", len(data_val))\n",
    "print(\"Total number of entries in the test dataset:       \", len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb906acd",
   "metadata": {},
   "source": [
    "##### Save and load datasets\n",
    "And save them using pandas in case we need them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5cd94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savetorch(onetensor, filename = '', sep = \",\", header = True):\n",
    "    \"\"\"\n",
    "    save a torch tensor as a csv file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(onetensor.numpy())\n",
    "    df.to_csv(path + filename, sep = sep, header = header, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c26c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "savetorch(data_train[:][1], 'traintarget.csv')\n",
    "savetorch(data_train[:][0], 'traincontext.csv')\n",
    "savetorch(data_val[:][1], 'valtarget.csv')\n",
    "savetorch(data_val[:][0], 'valcontext.csv')\n",
    "savetorch(data_test[:][1], 'testtarget.csv')\n",
    "savetorch(data_test[:][0], 'testcontext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e5611",
   "metadata": {},
   "source": [
    "To load the data from the csv files later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2b2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtotorch(fns): # filenames of the context, target data (list of strings)\n",
    "    \"\"\"\n",
    "    read csv files of contexts and target into torch tensors and get dataset\n",
    "    \"\"\"\n",
    "    context = pd.read_csv(path + fns[0])\n",
    "    target = pd.read_csv(path + fns[1])\n",
    "    contexts = torch.LongTensor(context.values)\n",
    "    targets = torch.LongTensor(target.values)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(contexts, targets)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "164dbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = readtotorch(['traincontext.csv', 'traintarget.csv'])\n",
    "data_val = readtotorch(['valcontext.csv', 'valtarget.csv'])\n",
    "data_test = readtotorch(['testcontext.csv', 'testtarget.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4fa25",
   "metadata": {},
   "source": [
    "### 2.1.4 Best n-gram model\n",
    "\n",
    "We define a function for computing accuracy to use for model selection and evaluation of our n-gram language model. We also make a model selection function where the best hyperparameters (variations of embedding size and learning rate) are chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "915881fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    \"\"\"\n",
    "    acc = (number of correct predictions)/(total number of predictions)\n",
    "    \"\"\"\n",
    "    hits = torch.sum(pred == target)\n",
    "    \n",
    "    return hits / len(target)\n",
    "\n",
    "def model_selection(lr_list,            # learning rates to be tested (list)\n",
    "                    embedding_size_list,# embedding size variations to be tested (list)\n",
    "                    data_train,         # training data (context/target)\n",
    "                    data_val,           # validation data (context/target)\n",
    "                    vocab_size,         # length of the vocabulary\n",
    "                    n_epochs = 20,      # number of epochs to train for (int)\n",
    "                    batch_size = 1024): # batch size \n",
    "    \"\"\"\n",
    "    checks validation accuracy of models with different hyperparameters (learning rate - lr, embedding size)\n",
    "    saves models state dictionaries\n",
    "    \n",
    "    returns:\n",
    "        best_model : the best hyperparameter combination\n",
    "    \"\"\"\n",
    "    # initialize variables to hold accuracy and best hyperparameters\n",
    "    best_acc = 0    \n",
    "    best_model = {}\n",
    "    count = 0\n",
    "    \n",
    "    # get train loader\n",
    "    torch.manual_seed(123)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = data_train,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = True)\n",
    "    # different hyperparameters\n",
    "    for lr in lr_list:\n",
    "        for embedding_size in embedding_size_list:\n",
    "\n",
    "            # make model and optimizer\n",
    "            model = NGramModel(vocab_size, embedding_size).to(device)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "            \n",
    "            print('\\n =========================================================\\n  Current parameters:')\n",
    "            print(f'lr: {lr}')  \n",
    "            print(f'embedding size: {embedding_size}')\n",
    "            \n",
    "            # training the model\n",
    "            model = train(model, n_epochs, train_loader, optimizer, loss_fn)\n",
    "            \n",
    "            # calculate validation accuracy\n",
    "            target = data_val[:][1]\n",
    "            pred = model(data_val[:][0])\n",
    "            pred = torch.argmax(pred, dim=1) # most likely word is prediction\n",
    "            acc = accuracy(pred, target.flatten())\n",
    "            print('accuracy:    {:.5f}'.format(acc))\n",
    "            \n",
    "            # save current trained model parameters for use later\n",
    "            torch.save(model.state_dict(), path + 'model_weights_' + str(count) + '.pth')\n",
    "            \n",
    "            # keep track of the best hyperparameters and validation accuracies\n",
    "            if acc > best_acc:\n",
    "                best_model['lr'] = lr\n",
    "                best_model['embedding size'] = embedding_size\n",
    "                best_model['accuracy'] = acc\n",
    "                best_model['number'] = count\n",
    "                best_acc = acc\n",
    "            # update number of trained models\n",
    "            count += 1\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7d329",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "Then we do model selection with different values for the learning rate and batch sizes (hyperparameters). A dictionary containing the hyperparameters giving the best validation accuracy is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7936eeda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.001\n",
      "embedding size: 16\n",
      "15:06:13.913206  |  Epoch 1  |  Training loss 6.90381\n",
      "15:09:21.441758  |  Epoch 5  |  Training loss 6.35261\n",
      "15:13:14.864000  |  Epoch 10  |  Training loss 5.86961\n",
      "15:17:08.945030  |  Epoch 15  |  Training loss 5.64995\n",
      "15:21:00.211583  |  Epoch 20  |  Training loss 5.52412\n",
      "15:24:50.606553  |  Epoch 25  |  Training loss 5.44262\n",
      "15:28:40.957439  |  Epoch 30  |  Training loss 5.38301\n",
      "accuracy:    0.11128\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.001\n",
      "embedding size: 32\n",
      "15:29:30.172449  |  Epoch 1  |  Training loss 6.90039\n",
      "15:32:44.290292  |  Epoch 5  |  Training loss 6.24776\n",
      "15:36:50.214439  |  Epoch 10  |  Training loss 5.78118\n",
      "15:40:53.084177  |  Epoch 15  |  Training loss 5.57084\n",
      "15:44:56.812510  |  Epoch 20  |  Training loss 5.44731\n",
      "15:48:59.109819  |  Epoch 25  |  Training loss 5.36115\n",
      "15:53:01.781760  |  Epoch 30  |  Training loss 5.29431\n",
      "accuracy:    0.12355\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.005\n",
      "embedding size: 16\n",
      "15:53:48.571994  |  Epoch 1  |  Training loss 6.57022\n",
      "15:56:56.977251  |  Epoch 5  |  Training loss 5.46776\n",
      "16:00:48.296243  |  Epoch 10  |  Training loss 5.26004\n",
      "16:04:39.310139  |  Epoch 15  |  Training loss 5.15787\n",
      "16:08:29.663584  |  Epoch 20  |  Training loss 5.08548\n",
      "16:12:21.942218  |  Epoch 25  |  Training loss 5.02545\n",
      "16:16:11.295247  |  Epoch 30  |  Training loss 4.97199\n",
      "accuracy:    0.14131\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr: 0.005\n",
      "embedding size: 32\n",
      "16:17:01.124442  |  Epoch 1  |  Training loss 6.63297\n",
      "16:20:25.046239  |  Epoch 5  |  Training loss 5.42948\n",
      "16:24:29.997369  |  Epoch 10  |  Training loss 5.16855\n",
      "16:28:33.921821  |  Epoch 15  |  Training loss 5.03491\n",
      "16:32:40.716429  |  Epoch 20  |  Training loss 4.93964\n",
      "16:36:44.280687  |  Epoch 25  |  Training loss 4.86341\n",
      "16:40:49.022202  |  Epoch 30  |  Training loss 4.79757\n",
      "accuracy:    0.16568\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.001, 0.005]\n",
    "embedding_size_list = [16, 32]\n",
    "n_epochs = 30\n",
    "loss_fn = nn.NLLLoss() # negative log likelihood\n",
    "\n",
    "print(f\"Training on device {device}.\")\n",
    "best_model = model_selection(lr_list, embedding_size_list, data_train, data_val, vocab_size, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76e269",
   "metadata": {},
   "source": [
    "##### Save and load best model hyperparameters\n",
    "We save the dictionary containing which model is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "479504f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'bestmodel_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b38ae2",
   "metadata": {},
   "source": [
    "And to load the dictionary back from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e30c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'bestmodel_dict.pkl', 'rb') as file_to_read:\n",
    "    best_model = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a73cf",
   "metadata": {},
   "source": [
    "##### Load trained model\n",
    "We then load the best models state dictionary to the n-gram language model architecture to get back the trained (best) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9483a942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# architecture\n",
    "embeddingmodel = NGramModel(vocab_size, best_model['embedding size']).to(device)\n",
    "\n",
    "# fill with trained parameters\n",
    "embeddingmodel.load_state_dict(torch.load(path + 'model_weights_' + str(best_model['number']) + '.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300eff3",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "We can then calculate training accuracy and evaluate (test accuracy) the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8701da33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:      0.15676\n",
      "Validation accuracy:    0.16568\n",
      "Test accuracy:          0.15568\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(dataset):\n",
    "    \"\"\"\n",
    "    take context/target datasets and calculate accuracy\n",
    "    \"\"\"\n",
    "    target = dataset[:][1]\n",
    "    pred = embeddingmodel(dataset[:][0])\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    \n",
    "    return accuracy(pred, target.flatten())\n",
    "\n",
    "acc_train = get_accuracy(data_train)\n",
    "\n",
    "# evaluate model (test accuracy)\n",
    "acc_test = get_accuracy(data_test)\n",
    "            \n",
    "print('Training accuracy:      {:.5f}'.format(acc_train))\n",
    "print('Validation accuracy:    {:.5f}'.format(best_model['accuracy']))\n",
    "print('Test accuracy:          {:.5f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254b77b",
   "metadata": {},
   "source": [
    "### 2.1.5 Cosine similarity\n",
    "\n",
    "We define functions to compute the cosine similarity matrix and to get n similar words to a word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c5a05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(E):\n",
    "    \"\"\"\n",
    "    calculate cosing similarity matrix from trained word embedding\n",
    "    \"\"\"\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    \n",
    "    sim_matrix = torch.zeros(vocab_size, vocab_size)\n",
    "    \n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            e_i = E[:][i].view((-1, 1))\n",
    "            e_j = E[:][j].view((-1, 1))\n",
    "            sim_matrix[i][j] = cos(e_i, e_j)\n",
    "            \n",
    "    return sim_matrix\n",
    "\n",
    "def n_most_similar_words(n, words, sim_matrix):\n",
    "    \"\"\"\n",
    "    Get the n most similar words to a list of words using the cosine\n",
    "    similarity matrix\n",
    "    \"\"\"\n",
    "    word_dict = {}\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        # index of word\n",
    "        index = vocab.__getitem__(w)\n",
    "        \n",
    "        # orders from lowest to highest similarity\n",
    "        similar_word_index = np.argsort(sim_matrix[:][index].detach().numpy())\n",
    "        \n",
    "        # get highest to lowest\n",
    "        similar_word_index = np.flip(similar_word_index)[1:n+1]\n",
    "        \n",
    "        similar_words = vocab.lookup_tokens(similar_word_index)\n",
    "        \n",
    "        word_dict[w] = similar_words\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fd5e9",
   "metadata": {},
   "source": [
    "#### Cosine similarity matrix\n",
    "We get the cosine similarity matrix from the trained embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd52457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0896, -0.1920,  ...,  0.1734, -0.0356, -0.0967],\n",
       "        [ 0.0896,  1.0000,  0.1909,  ...,  0.1808, -0.1294,  0.0371],\n",
       "        [-0.1920,  0.1909,  1.0000,  ...,  0.2394,  0.1400,  0.0113],\n",
       "        ...,\n",
       "        [ 0.1734,  0.1808,  0.2394,  ...,  1.0000,  0.2037,  0.1586],\n",
       "        [-0.0356, -0.1294,  0.1400,  ...,  0.2037,  1.0000,  0.0964],\n",
       "        [-0.0967,  0.0371,  0.0113,  ...,  0.1586,  0.0964,  1.0000]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding matrix (weights of embedding)\n",
    "E = embeddingmodel.embedding.weight\n",
    "\n",
    "sim_matrix = similarity_matrix(E)\n",
    "\n",
    "sim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e682d",
   "metadata": {},
   "source": [
    "#### Similar words\n",
    "And use it to find the ten most similar words to a selection of words in our vocabulary according to the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3a432d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = ['woman', 'have', 'be', 'yes', 'no', 'house', 'red', 'blue', 'two']\n",
    "n = 10\n",
    "\n",
    "word_dict = n_most_similar_words(n, test_words, sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25e5de52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar words to: woman\n",
      "as\n",
      "during\n",
      "plan\n",
      "listening\n",
      "understand\n",
      "bring\n",
      "move\n",
      "street\n",
      "fixed\n",
      "same\n",
      "\n",
      "Similar words to: have\n",
      "regard\n",
      "asked\n",
      "last\n",
      "left\n",
      "act\n",
      "continued\n",
      "since\n",
      "sure\n",
      "several\n",
      "drawing\n",
      "\n",
      "Similar words to: be\n",
      "honor\n",
      "pleasant\n",
      "faces\n",
      "tears\n",
      "bit\n",
      "m\n",
      "means\n",
      "other\n",
      "won\n",
      "found\n",
      "\n",
      "Similar words to: yes\n",
      "important\n",
      "m\n",
      "common\n",
      "land\n",
      "grown\n",
      "nature\n",
      "away\n",
      "will\n",
      "its\n",
      "subject\n",
      "\n",
      "Similar words to: no\n",
      "we\n",
      "they\n",
      "further\n",
      "myself\n",
      "than\n",
      "from\n",
      "thinking\n",
      "who\n",
      "least\n",
      "am\n",
      "\n",
      "Similar words to: house\n",
      "just\n",
      "simple\n",
      "murder\n",
      "talked\n",
      "ever\n",
      "front\n",
      "probably\n",
      "hat\n",
      "fresh\n",
      "passion\n",
      "\n",
      "Similar words to: red\n",
      "affair\n",
      "stand\n",
      "seen\n",
      "subject\n",
      "how\n",
      "repeated\n",
      "warm\n",
      "whose\n",
      "grown\n",
      "sake\n",
      "\n",
      "Similar words to: blue\n",
      "wish\n",
      "caught\n",
      "yourself\n",
      "law\n",
      "lies\n",
      "should\n",
      "large\n",
      "especially\n",
      "express\n",
      "say\n",
      "\n",
      "Similar words to: two\n",
      "lying\n",
      "finished\n",
      "really\n",
      "quick\n",
      "galloped\n",
      "matter\n",
      "heard\n",
      "able\n",
      "spent\n",
      "command\n"
     ]
    }
   ],
   "source": [
    "for w in test_words:\n",
    "    print('\\nSimilar words to: ' + w)\n",
    "    for word in word_dict[w]:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e191eaa7",
   "metadata": {},
   "source": [
    "### 2.1.6 To visualize embedding space\n",
    "\n",
    "We save the vocabulary and the corresponding values in embedding space as tsv-files for uploading to https://projector.tensorflow.org/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e589cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "listofwords = vocab.lookup_tokens([i for i in range(vocab_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "335364e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#savetorch(vocab, 'vocab.tsv', sep=\"\\t\", header=False) # words in vocabulary\n",
    "df = pd.DataFrame(listofwords)\n",
    "df.to_csv(path+'vocab.tsv', sep='\\t', header=False, index=False)\n",
    "savetorch(E, 'embedding.tsv', sep='\\t', header=False) # corresponding values in embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a91265",
   "metadata": {},
   "source": [
    "See the pdf report for the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322002f1",
   "metadata": {},
   "source": [
    "## 2.2 Conjugating be and have\n",
    "\n",
    "We are to predict *be* and *have* conjugation given the context around the missing target.Possible conjugations are *be, am, are, is, was, were, been, being, have, has, had, having*, corresponding to 12 classes (integers 0 to 11).\n",
    "\n",
    "#### Making context/target datasets\n",
    "We make the context/target datasets where the targets are integers between 0 and 11. We use a context size of four, meaning two words before and two words after the target word (same as when training the embedding in Task 2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e24ba772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conjugate_dataset(text,              # all words in the dataset\n",
    "                             vocab,             # vocabulary\n",
    "                             conjugate_list,    # list of conjugations of be and have\n",
    "                             context_size = 4): # effectively two words before and two words after target\n",
    "\n",
    "    \"\"\"\n",
    "    Create context/target dataset with integer features, based on given target words\n",
    "    \"\"\"\n",
    "    n_text = len(text)\n",
    "    context_size = context_size//2\n",
    "    \n",
    "    # Transform the text to a list of integers.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size, n_text - context_size):\n",
    "        if text[i] in(conjugate_list):\n",
    "            if txt[i] in specials: # see above - removing ['<unk>',',','.','!','?','(',')','-'] from datasets\n",
    "                continue\n",
    "            # get context words before and after\n",
    "            contexts.append(torch.LongTensor([txt[i-context_size+j] for j in range(context_size*2+1) if j!=context_size]))\n",
    "            # map targets to integers 0-11\n",
    "            targets.append(conjugate_list.index(text[i]))\n",
    "        \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.LongTensor(targets)\n",
    "    targets = torch.unsqueeze(targets, dim=1)\n",
    "    data = torch.utils.data.TensorDataset(contexts, targets)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bfaf94",
   "metadata": {},
   "source": [
    "We use the above function to create datasets containing the conjugations of *be* and *have*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1b48928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the training dataset:    61961\n",
      "Total number of entries in the validation dataset:  2590\n",
      "Total number of entries in the test dataset:        4502\n"
     ]
    }
   ],
   "source": [
    "# conjugations of be and have\n",
    "conjugate_list = ['be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "\n",
    "# new context/target datasets\n",
    "data_train_2 = create_conjugate_dataset(words_train, vocab, conjugate_list)\n",
    "data_val_2 = create_conjugate_dataset(words_val, vocab, conjugate_list)\n",
    "data_test_2 = create_conjugate_dataset(words_test, vocab, conjugate_list)\n",
    "\n",
    "print(\"Total number of entries in the training dataset:   \", len(data_train_2))\n",
    "print(\"Total number of entries in the validation dataset: \", len(data_val_2))\n",
    "print(\"Total number of entries in the test dataset:       \", len(data_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702320b",
   "metadata": {},
   "source": [
    "##### Save and load datasets\n",
    "And save them using pandas in case we need them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c489c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "savetorch(data_train_2[:][1], 'traintarget_conj.csv')\n",
    "savetorch(data_train_2[:][0], 'traincontext_conj.csv')\n",
    "savetorch(data_val_2[:][1], 'valtarget_conj.csv')\n",
    "savetorch(data_val_2[:][0], 'valcontext_conj.csv')\n",
    "savetorch(data_test_2[:][1], 'testtarget_conj.csv')\n",
    "savetorch(data_test_2[:][0], 'testcontext_conj.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69556e32",
   "metadata": {},
   "source": [
    "To load the data from the csv files later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61fecf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_2 = readtotorch(['traincontext_conj.csv', 'traintarget_conj.csv'])\n",
    "data_val_2 = readtotorch(['valcontext_conj.csv', 'valtarget_conj.csv'])\n",
    "data_test_2 = readtotorch(['testcontext_conj.csv', 'testtarget_conj.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2aafb",
   "metadata": {},
   "source": [
    "### 2.2.1 MLP and RNN to predict conjugation\n",
    "\n",
    "We are to look at two different architectures for predicting the conjugations:\n",
    "- Multilayer perception (MLP)\n",
    "- Recurrent neural network (RNN)\n",
    "\n",
    "#### MLP\n",
    "\n",
    "We define an MLP architecture based on the embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08460d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding,         # word embedding (frozen, not to be trained)\n",
    "                 context_size = 4): # (two words before and two words after target)\n",
    "        super().__init__()\n",
    "        \n",
    "        embedding_size = embedding.weight.shape[1]\n",
    "        \n",
    "        # frozen embedding layer\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # three fuly connected layers\n",
    "        self.fc1 = nn.Linear(embedding_size*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = torch.flatten(embeds, 1)\n",
    "        out = torch.relu(self.fc1(embeds))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf57b8d",
   "metadata": {},
   "source": [
    "#### RNN\n",
    "\n",
    "We define a RNN architecture based on the embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9c6548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding,        # word embedding (frozen, not to be trained)\n",
    "                 hidden_size = 12, # size of hidden layer \n",
    "                 n_layers = 2):    # number of layers\n",
    "        super().__init__()\n",
    "        \n",
    "        embedding_size = embedding.weight.shape[1]\n",
    "        \n",
    "        # frozen embedding layer\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # RNN layer and one fully connected layer for output\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        _, hidden = self.rnn(embeds)\n",
    "        out = nn.ReLU()(hidden[-1])\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a4e7b",
   "metadata": {},
   "source": [
    "We then define the function for training our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af4a77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conjugate(model,        # model architecture to train\n",
    "                    n_epochs,     # number of epochs to train for\n",
    "                    train_loader, # dataloader\n",
    "                    optimizer,    # optimizer\n",
    "                    loss_fn):     # loss function\n",
    "                \n",
    "    \"\"\"\n",
    "    Function for training the model\n",
    "    \"\"\"\n",
    "    # get the number of batches\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        for contexts, targets in train_loader:\n",
    "            model.zero_grad()\n",
    "            output = model(contexts)\n",
    "            \n",
    "            loss = loss_fn(output, targets.flatten())\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        losses.append(epoch_loss)\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, epoch_loss/n_batches))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e4098",
   "metadata": {},
   "source": [
    "### 2.2.2 Best conjugation predictor\n",
    "\n",
    "We want to do model selection to find the best model for predicting conjugatins of *be* and *have*. We define a function for model selection based on validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6747978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_model_selection(lr_list,            # learning rates to be tested (list)\n",
    "                              hidden_size_list,   # hidden size variations to be tested for RNN (list)\n",
    "                              data_train,         # training data (context/target)\n",
    "                              data_val,           # validation data (context/target)\n",
    "                              embedding,          # word embedding (frozen, not to be trained)\n",
    "                              n_epochs = 30,      # number of epochs to train for (int)\n",
    "                              batch_size = 1024): # batch size \n",
    "    \"\"\"\n",
    "    checks validation accuracy of models with different architectures and hyperparameters\n",
    "    saves models state dictionaries\n",
    "    \n",
    "    returns:\n",
    "        best_model : the best hyperparameter combination\n",
    "    \"\"\"\n",
    "    # initialize variables to hold accuracy and best hyperparameters\n",
    "    best_acc = 0\n",
    "    best_model = {}\n",
    "    count = 0\n",
    "    \n",
    "    # get train loader\n",
    "    torch.manual_seed(123)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = data_train,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = True)\n",
    "    # different hyperparameters\n",
    "    for lr in lr_list:\n",
    "        \n",
    "        # ---------------------------- variations on RNN architecture ----------------------------------\n",
    "        for hidden_size in hidden_size_list:\n",
    "           \n",
    "            model = RNN(embedding, hidden_size)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "            print('\\n =========================================================\\n  Current parameters:')\n",
    "            print('---- RNN model ----')\n",
    "            print(f'lr:             {lr}')  \n",
    "            print(f'hidden size:     {hidden_size}')\n",
    "            \n",
    "            # train model\n",
    "            train_start = datetime.now()\n",
    "            model = train_conjugate(model, n_epochs, train_loader, optimizer, loss_fn)\n",
    "            \n",
    "            # get validation accuracy\n",
    "            target = data_val[:][1]\n",
    "            pred = model(data_val[:][0])\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            acc = accuracy(pred, target.flatten())\n",
    "            print('accuracy:        {:.5f}'.format(acc))\n",
    "            \n",
    "            # get time it takes to train RNN\n",
    "            train_stop = datetime.now()\n",
    "            train_delta = train_stop - train_start\n",
    "            sec = train_delta.seconds\n",
    "            print(f'training time:   {sec} seconds')\n",
    "            \n",
    "            # save current trained model parameters for use later\n",
    "            torch.save(model.state_dict(), path + 'model_weights_conjugate_' + str(count) + '.pth')\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_model['lr'] = lr\n",
    "                best_model['hidden size'] = hidden_size\n",
    "                best_model['accuracy'] = acc\n",
    "                best_model['model'] = 'RNN'\n",
    "                best_model['number'] = count\n",
    "                best_model['seconds'] = sec\n",
    "                best_acc = acc\n",
    "            \n",
    "            # update number of trained models\n",
    "            count += 1\n",
    "        \n",
    "        # ------------------------------- MLP architecture -------------------------------------------\n",
    "        # get model and optimizer\n",
    "        model = MLP(embedding)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "        print('\\n =========================================================\\n  Current parameters:')\n",
    "        print('---- MLP model ----')\n",
    "        print(f'lr:             {lr}')  \n",
    "\n",
    "        # train model\n",
    "        train_start = datetime.now()\n",
    "        model = train_conjugate(model, n_epochs, train_loader, optimizer, loss_fn)\n",
    "\n",
    "        # get validation accuracy\n",
    "        target = data_val[:][1]\n",
    "        pred = model(data_val[:][0])\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        acc = accuracy(pred, target.flatten())\n",
    "        print('accuracy:        {:.5f}'.format(acc))\n",
    "        \n",
    "        # get time it takes to train MLP\n",
    "        train_stop = datetime.now()\n",
    "        train_delta = train_stop - train_start\n",
    "        sec = train_delta.seconds\n",
    "        print(f'training time:   {sec} seconds')\n",
    "            \n",
    "        # save current trained model parameters for use later\n",
    "        torch.save(model.state_dict(), path + 'model_weights_conjugate_' + str(count) + '.pth')\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_model['lr'] = lr\n",
    "            best_model['hidden size'] = None\n",
    "            best_model['accuracy'] = acc\n",
    "            best_model['model'] = 'MLP'\n",
    "            best_model['number'] = count\n",
    "            best_model['seconds'] = sec\n",
    "            best_acc = acc\n",
    "\n",
    "        # update number of trained models\n",
    "        count += 1\n",
    "                   \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131d7ac",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "Then we do model selection with different hyperparameter values. We train two MLP architectures with different learning rates, and four RNN architectures where we vary both learning rate and number of hidden units. A dictionary containing the hyperparameters giving the best validation accuracy is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb5fa4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "---- RNN model ----\n",
      "lr:             0.001\n",
      "hidden size:     12\n",
      "17:50:45.077769  |  Epoch 1  |  Training loss 2.51716\n",
      "17:50:47.047290  |  Epoch 5  |  Training loss 2.47296\n",
      "17:50:49.513164  |  Epoch 10  |  Training loss 2.43025\n",
      "17:50:51.945862  |  Epoch 15  |  Training loss 2.39532\n",
      "17:50:54.421655  |  Epoch 20  |  Training loss 2.36578\n",
      "17:50:56.856322  |  Epoch 25  |  Training loss 2.34024\n",
      "17:50:59.452014  |  Epoch 30  |  Training loss 2.31841\n",
      "17:51:02.012333  |  Epoch 35  |  Training loss 2.29896\n",
      "17:51:04.468135  |  Epoch 40  |  Training loss 2.28256\n",
      "17:51:06.965399  |  Epoch 45  |  Training loss 2.26818\n",
      "17:51:09.496510  |  Epoch 50  |  Training loss 2.25628\n",
      "17:51:12.015567  |  Epoch 55  |  Training loss 2.24555\n",
      "17:51:14.456674  |  Epoch 60  |  Training loss 2.23683\n",
      "17:51:16.978050  |  Epoch 65  |  Training loss 2.22916\n",
      "17:51:19.418716  |  Epoch 70  |  Training loss 2.22277\n",
      "17:51:21.899170  |  Epoch 75  |  Training loss 2.21728\n",
      "17:51:24.381378  |  Epoch 80  |  Training loss 2.21247\n",
      "17:51:26.830192  |  Epoch 85  |  Training loss 2.20794\n",
      "17:51:29.316060  |  Epoch 90  |  Training loss 2.20430\n",
      "17:51:31.765720  |  Epoch 95  |  Training loss 2.20070\n",
      "17:51:34.252096  |  Epoch 100  |  Training loss 2.19776\n",
      "accuracy:        0.25135\n",
      "training time:   49 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "---- RNN model ----\n",
      "lr:             0.001\n",
      "hidden size:     24\n",
      "17:51:34.929652  |  Epoch 1  |  Training loss 2.47899\n",
      "17:51:37.640488  |  Epoch 5  |  Training loss 2.45780\n",
      "17:51:41.057354  |  Epoch 10  |  Training loss 2.43341\n",
      "17:51:44.445906  |  Epoch 15  |  Training loss 2.41068\n",
      "17:51:47.866675  |  Epoch 20  |  Training loss 2.38934\n",
      "17:51:51.246950  |  Epoch 25  |  Training loss 2.36904\n",
      "17:51:54.666404  |  Epoch 30  |  Training loss 2.34986\n",
      "17:51:58.063518  |  Epoch 35  |  Training loss 2.33175\n",
      "17:52:01.506477  |  Epoch 40  |  Training loss 2.31436\n",
      "17:52:04.925245  |  Epoch 45  |  Training loss 2.29774\n",
      "17:52:08.301965  |  Epoch 50  |  Training loss 2.28259\n",
      "17:52:11.719019  |  Epoch 55  |  Training loss 2.26752\n",
      "17:52:15.093945  |  Epoch 60  |  Training loss 2.25404\n",
      "17:52:18.504517  |  Epoch 65  |  Training loss 2.24244\n",
      "17:52:21.876606  |  Epoch 70  |  Training loss 2.23137\n",
      "17:52:25.282991  |  Epoch 75  |  Training loss 2.22222\n",
      "17:52:28.653090  |  Epoch 80  |  Training loss 2.21313\n",
      "17:52:32.061421  |  Epoch 85  |  Training loss 2.20608\n",
      "17:52:35.476247  |  Epoch 90  |  Training loss 2.19955\n",
      "17:52:38.849713  |  Epoch 95  |  Training loss 2.19433\n",
      "17:52:42.263425  |  Epoch 100  |  Training loss 2.18895\n",
      "accuracy:        0.25251\n",
      "training time:   68 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "---- MLP model ----\n",
      "lr:             0.001\n",
      "17:52:42.871308  |  Epoch 1  |  Training loss 2.51036\n",
      "17:52:45.285417  |  Epoch 5  |  Training loss 2.48400\n",
      "17:52:48.332742  |  Epoch 10  |  Training loss 2.45277\n",
      "17:52:51.340956  |  Epoch 15  |  Training loss 2.42140\n",
      "17:52:54.388601  |  Epoch 20  |  Training loss 2.38861\n",
      "17:52:57.437093  |  Epoch 25  |  Training loss 2.35399\n",
      "17:53:00.494298  |  Epoch 30  |  Training loss 2.31778\n",
      "17:53:03.536425  |  Epoch 35  |  Training loss 2.28225\n",
      "17:53:06.559596  |  Epoch 40  |  Training loss 2.24931\n",
      "17:53:09.601380  |  Epoch 45  |  Training loss 2.22206\n",
      "17:53:12.607994  |  Epoch 50  |  Training loss 2.20010\n",
      "17:53:15.674053  |  Epoch 55  |  Training loss 2.18324\n",
      "17:53:18.678422  |  Epoch 60  |  Training loss 2.17031\n",
      "17:53:21.717337  |  Epoch 65  |  Training loss 2.15999\n",
      "17:53:24.757920  |  Epoch 70  |  Training loss 2.15129\n",
      "17:53:27.761924  |  Epoch 75  |  Training loss 2.14380\n",
      "17:53:30.801298  |  Epoch 80  |  Training loss 2.13731\n",
      "17:53:33.800564  |  Epoch 85  |  Training loss 2.13153\n",
      "17:53:36.839850  |  Epoch 90  |  Training loss 2.12565\n",
      "17:53:39.840034  |  Epoch 95  |  Training loss 2.11927\n",
      "17:53:42.876756  |  Epoch 100  |  Training loss 2.11320\n",
      "accuracy:        0.26255\n",
      "training time:   60 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "---- RNN model ----\n",
      "lr:             0.005\n",
      "hidden size:     12\n",
      "17:53:43.384794  |  Epoch 1  |  Training loss 2.49184\n",
      "17:53:45.410879  |  Epoch 5  |  Training loss 2.40251\n",
      "17:53:47.874739  |  Epoch 10  |  Training loss 2.32285\n",
      "17:53:50.379119  |  Epoch 15  |  Training loss 2.26746\n",
      "17:53:52.842890  |  Epoch 20  |  Training loss 2.23143\n",
      "17:53:55.341723  |  Epoch 25  |  Training loss 2.20984\n",
      "17:53:57.811342  |  Epoch 30  |  Training loss 2.19630\n",
      "17:54:00.350768  |  Epoch 35  |  Training loss 2.18753\n",
      "17:54:02.812953  |  Epoch 40  |  Training loss 2.18150\n",
      "17:54:05.393156  |  Epoch 45  |  Training loss 2.17668\n",
      "17:54:07.895609  |  Epoch 50  |  Training loss 2.17323\n",
      "17:54:10.357106  |  Epoch 55  |  Training loss 2.16994\n",
      "17:54:12.856281  |  Epoch 60  |  Training loss 2.16704\n",
      "17:54:15.320482  |  Epoch 65  |  Training loss 2.16387\n",
      "17:54:17.827966  |  Epoch 70  |  Training loss 2.16215\n",
      "17:54:20.287081  |  Epoch 75  |  Training loss 2.15926\n",
      "17:54:22.786239  |  Epoch 80  |  Training loss 2.15643\n",
      "17:54:25.283114  |  Epoch 85  |  Training loss 2.15332\n",
      "17:54:27.743335  |  Epoch 90  |  Training loss 2.15058\n",
      "17:54:30.242897  |  Epoch 95  |  Training loss 2.14782\n",
      "17:54:32.700676  |  Epoch 100  |  Training loss 2.14471\n",
      "accuracy:        0.25753\n",
      "training time:   49 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "---- RNN model ----\n",
      "lr:             0.005\n",
      "hidden size:     24\n",
      "17:54:33.418239  |  Epoch 1  |  Training loss 2.49961\n",
      "17:54:36.126897  |  Epoch 5  |  Training loss 2.31086\n",
      "17:54:39.499565  |  Epoch 10  |  Training loss 2.20384\n",
      "17:54:42.913110  |  Epoch 15  |  Training loss 2.18084\n",
      "17:54:46.285998  |  Epoch 20  |  Training loss 2.17261\n",
      "17:54:49.693167  |  Epoch 25  |  Training loss 2.16692\n",
      "17:54:53.099525  |  Epoch 30  |  Training loss 2.16297\n",
      "17:54:56.472112  |  Epoch 35  |  Training loss 2.15815\n",
      "17:54:59.917411  |  Epoch 40  |  Training loss 2.15370\n",
      "17:55:03.286569  |  Epoch 45  |  Training loss 2.14797\n",
      "17:55:06.697318  |  Epoch 50  |  Training loss 2.14081\n",
      "17:55:10.065848  |  Epoch 55  |  Training loss 2.13249\n",
      "17:55:13.478092  |  Epoch 60  |  Training loss 2.12235\n",
      "17:55:16.887340  |  Epoch 65  |  Training loss 2.10983\n",
      "17:55:20.256849  |  Epoch 70  |  Training loss 2.09533\n",
      "17:55:23.666275  |  Epoch 75  |  Training loss 2.07961\n",
      "17:55:27.037436  |  Epoch 80  |  Training loss 2.06316\n",
      "17:55:30.446491  |  Epoch 85  |  Training loss 2.04560\n",
      "17:55:33.818662  |  Epoch 90  |  Training loss 2.02758\n",
      "17:55:37.225887  |  Epoch 95  |  Training loss 2.00923\n",
      "17:55:40.597223  |  Epoch 100  |  Training loss 1.99184\n",
      "accuracy:        0.33320\n",
      "training time:   67 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "---- MLP model ----\n",
      "lr:             0.005\n",
      "17:55:41.242330  |  Epoch 1  |  Training loss 2.42446\n",
      "17:55:43.664490  |  Epoch 5  |  Training loss 2.27289\n",
      "17:55:46.709890  |  Epoch 10  |  Training loss 2.16677\n",
      "17:55:49.713931  |  Epoch 15  |  Training loss 2.11797\n",
      "17:55:52.748474  |  Epoch 20  |  Training loss 2.07867\n",
      "17:55:55.745173  |  Epoch 25  |  Training loss 2.03744\n",
      "17:55:58.819241  |  Epoch 30  |  Training loss 1.98948\n",
      "17:56:01.823006  |  Epoch 35  |  Training loss 1.93822\n",
      "17:56:04.859074  |  Epoch 40  |  Training loss 1.88786\n",
      "17:56:07.893277  |  Epoch 45  |  Training loss 1.84180\n",
      "17:56:10.886752  |  Epoch 50  |  Training loss 1.79904\n",
      "17:56:13.931070  |  Epoch 55  |  Training loss 1.75932\n",
      "17:56:16.941040  |  Epoch 60  |  Training loss 1.72372\n",
      "17:56:19.973254  |  Epoch 65  |  Training loss 1.69297\n",
      "17:56:22.965293  |  Epoch 70  |  Training loss 1.66518\n",
      "17:56:25.995352  |  Epoch 75  |  Training loss 1.64135\n",
      "17:56:28.986441  |  Epoch 80  |  Training loss 1.62061\n",
      "17:56:32.021981  |  Epoch 85  |  Training loss 1.60240\n",
      "17:56:35.073327  |  Epoch 90  |  Training loss 1.58596\n",
      "17:56:38.066236  |  Epoch 95  |  Training loss 1.57153\n",
      "17:56:41.099686  |  Epoch 100  |  Training loss 1.55715\n",
      "accuracy:        0.45019\n",
      "training time:   60 seconds\n"
     ]
    }
   ],
   "source": [
    "wordembedding = embeddingmodel.embedding\n",
    "\n",
    "# hyperparameters\n",
    "lr_list = [0.001, 0.005]\n",
    "hidden_size_list = [12, 24] # for RNN\n",
    "\n",
    "n_epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss() # Cross-entropy loss\n",
    "\n",
    "print(f\"Training on device {device}.\")\n",
    "best_model_2 = conjugate_model_selection(lr_list, hidden_size_list, data_train_2, data_val_2, wordembedding, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ef22f",
   "metadata": {},
   "source": [
    "##### Save and load best model hyperparameters\n",
    "We save the dictionary containing which model is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "246e4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'bestmodel_dict_conjugate.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model_2, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be833c7",
   "metadata": {},
   "source": [
    "And to load the dictionary back from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e578296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'bestmodel_dict_conjugate.pkl', 'rb') as file_to_read:\n",
    "    best_model_2 = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079603c",
   "metadata": {},
   "source": [
    "##### Load trained model\n",
    "We then load the best models state dictionary to correct model architecture to get back the trained (best) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a707cbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordembedding = embeddingmodel.embedding\n",
    "\n",
    "# architecture\n",
    "if best_model_2['model'] == 'RNN':\n",
    "    conjugatemodel = model = RNN(wordembedding, best_model_2['hidden size'])\n",
    "elif best_model_2['model'] == 'MLP':\n",
    "    conjugatemodel = MLP(wordembedding)\n",
    "\n",
    "# fill with trained parameters\n",
    "conjugatemodel.load_state_dict(torch.load(path + 'model_weights_conjugate_' + str(best_model_2['number']) + '.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aaef40",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "We can then calculate training accuracy and evaluate (test accuracy) the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7d56c09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:      0.44031\n",
      "Validation accuracy:    0.45019\n",
      "Test accuracy:          0.24656\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_2(dataset):\n",
    "    \"\"\"\n",
    "    take context/target datasets and calculate accuracy\n",
    "    \"\"\"\n",
    "    target = dataset[:][1]\n",
    "    pred = conjugatemodel(dataset[:][0])\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    \n",
    "    return accuracy(pred, target.flatten())\n",
    "\n",
    "acc_train = get_accuracy_2(data_train_2)\n",
    "\n",
    "# evaluate model (test accuracy)\n",
    "acc_test = get_accuracy_2(data_test_2)\n",
    "            \n",
    "print('Training accuracy:      {:.5f}'.format(acc_train))\n",
    "print('Validation accuracy:    {:.5f}'.format(best_model_2['accuracy']))\n",
    "print('Test accuracy:          {:.5f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b6579",
   "metadata": {},
   "source": [
    "## 2.3 Text generation\n",
    "\n",
    "We use our trained word embedding to define an RNN architecture to predict the next word in a sequence given a context before the target.\n",
    "\n",
    "#### Making context/target datasets\n",
    "\n",
    "For this task we need new context/target datasets containing context words before the target word (we choose three context words). We define a function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0d0df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_3(text,              # all words in dataset\n",
    "                     vocab,             # vocabulary\n",
    "                     context_size = 3): # number of context words before the targets\n",
    "    \"\"\"\n",
    "    Create context/target dataset with integer features\n",
    "    \"\"\"\n",
    "    \n",
    "    # get dataset\n",
    "    n_text = len(text)\n",
    "    \n",
    "    # transform the text as a list of integers\n",
    "    txt = [vocab[w] for w in text]\n",
    "    \n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(context_size, n_text - context_size):\n",
    "        if txt[i] in specials: # see above - removing ['<unk>',',','.','!','?','(',')','-'] from datasets\n",
    "            continue\n",
    "        contexts.append(torch.LongTensor([txt[i - context_size + j] for j in range(context_size)]))\n",
    "        targets.append(txt[i])\n",
    "        \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.LongTensor(targets)\n",
    "    targets = torch.unsqueeze(targets, dim=1)\n",
    "    data = torch.utils.data.TensorDataset(contexts, targets) \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e749d",
   "metadata": {},
   "source": [
    "We create the context/target datasets (training, validation, and test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92e4eb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the training dataset:    949665\n",
      "Total number of entries in the validation dataset:  34299\n",
      "Total number of entries in the test dataset:        86266\n"
     ]
    }
   ],
   "source": [
    "data_train_3 = create_dataset_3(words_train, vocab)\n",
    "data_val_3 = create_dataset_3(words_val, vocab)\n",
    "data_test_3 = create_dataset_3(words_test, vocab)\n",
    "\n",
    "print(\"Total number of entries in the training dataset:   \", len(data_train_3))\n",
    "print(\"Total number of entries in the validation dataset: \", len(data_val_3))\n",
    "print(\"Total number of entries in the test dataset:       \", len(data_test_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9f1b9",
   "metadata": {},
   "source": [
    "##### Save and load datasets\n",
    "And save them using pandas in case we need them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1f0a1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "savetorch(data_train_3[:][1], 'traintarget_3.csv')\n",
    "savetorch(data_train_3[:][0], 'traincontext_3.csv')\n",
    "savetorch(data_val_3[:][1], 'valtarget_3.csv')\n",
    "savetorch(data_val_3[:][0], 'valcontext_3.csv')\n",
    "savetorch(data_test_3[:][1], 'testtarget_3.csv')\n",
    "savetorch(data_test_3[:][0], 'testcontext_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6c657",
   "metadata": {},
   "source": [
    "To load the data from the csv files later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7394b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_3 = readtotorch(['traincontext_3.csv', 'traintarget_3.csv'])\n",
    "data_val_3 = readtotorch(['valcontext_3.csv', 'valtarget_3.csv'])\n",
    "data_test_3 = readtotorch(['testcontext_3.csv', 'testtarget_3.csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6dfc4",
   "metadata": {},
   "source": [
    "### 2.3.1 RNN for word prediction\n",
    "\n",
    "We can now define an RNN architecture based on the word embedding that is to predict the following words given a context of previous words. The model can be trained using the train_conjugate() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a7cb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding,        # word embedding (frozen, not to be trained)\n",
    "                 vocab_size,       # size of vocabulary (output)\n",
    "                 hidden_size = 12, # size of hidden layer \n",
    "                 n_layers = 2):    # number of layers\n",
    "        super().__init__()\n",
    "        \n",
    "        embedding_dim = embedding.weight.shape[1]\n",
    "        \n",
    "        # frozen embedding layer\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # RNN layer and one fully connected layer for output\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first = True)\n",
    "        self.fc1 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        _, hidden = self.rnn(embeds)\n",
    "        out = nn.ReLU()(hidden[-1])\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd99cc",
   "metadata": {},
   "source": [
    "### 2.3.2 Best next word predictor\n",
    "\n",
    "We want to do model selection to find the best model for predicting the next word based on previous words. We define a function for model selection based on validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56c217dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_model_selection(lr_list,            # learning rates to be tested (list)\n",
    "                         hidden_size_list,   # hidden size variations to be tested for RNN (list)\n",
    "                         data_train,         # training data (context/target)\n",
    "                         data_val,           # validation data (context/target)\n",
    "                         embedding,          # word embedding (frozen, not to be trained)\n",
    "                         vocab_size,         # length of the vocabulary\n",
    "                         n_epochs = 30,      # number of epochs to train for (int)\n",
    "                         batch_size = 1024): # batch size \n",
    "    \"\"\"\n",
    "    checks validation accuracy of models with different architectures and hyperparameters\n",
    "    saves models state dictionaries\n",
    "    \n",
    "    returns:\n",
    "        best_model : the best hyperparameter combination\n",
    "    \"\"\"\n",
    "    # initialize variables to hold accuracy and best hyperparameters\n",
    "    best_acc = 0\n",
    "    best_model = {}\n",
    "    count = 0\n",
    "    \n",
    "    # get train loader\n",
    "    torch.manual_seed(123)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = data_train,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = True)\n",
    "    # different hyperparameters\n",
    "    for lr in lr_list:\n",
    "        for hidden_size in hidden_size_list:\n",
    "           \n",
    "            model = myRNN(embedding, vocab_size, hidden_size)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr)\n",
    "\n",
    "            print('\\n =========================================================\\n  Current parameters:')\n",
    "            print(f'lr:             {lr}')  \n",
    "            print(f'hidden size:     {hidden_size}')\n",
    "            \n",
    "            # train model\n",
    "            train_start = datetime.now()\n",
    "            model = train_conjugate(model, n_epochs, train_loader, optimizer, loss_fn)\n",
    "            \n",
    "            # get validation accuracy\n",
    "            target = data_val[:][1]\n",
    "            pred = model(data_val[:][0])\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            acc = accuracy(pred, target.flatten())\n",
    "            print('accuracy:        {:.5f}'.format(acc))\n",
    "            \n",
    "            # get time it takes to train RNN\n",
    "            train_stop = datetime.now()\n",
    "            train_delta = train_stop - train_start\n",
    "            sec = train_delta.seconds\n",
    "            print(f'training time:   {sec} seconds')\n",
    "            \n",
    "            # save current trained model parameters for use later\n",
    "            torch.save(model.state_dict(), path + 'model_weights_next_' + str(count) + '.pth')\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_model['lr'] = lr\n",
    "                best_model['hidden size'] = hidden_size\n",
    "                best_model['accuracy'] = acc\n",
    "                best_model['number'] = count\n",
    "                best_model['seconds'] = sec\n",
    "                best_acc = acc\n",
    "            \n",
    "            # update number of trained models\n",
    "            count += 1\n",
    "                   \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0f743",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "Then we do model selection with different hyperparameter values. We train two MLP architectures with different learning rates, and four RNN architectures where we vary both learning rate and number of hidden units. A dictionary containing the hyperparameters giving the best validation accuracy is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fe2075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr:             0.001\n",
      "hidden size:     12\n",
      "18:34:57.136644  |  Epoch 1  |  Training loss 6.98308\n",
      "18:35:57.353689  |  Epoch 5  |  Training loss 6.85885\n",
      "18:37:13.725189  |  Epoch 10  |  Training loss 6.66778\n",
      "18:38:30.294013  |  Epoch 15  |  Training loss 6.40180\n",
      "18:39:46.353042  |  Epoch 20  |  Training loss 6.17225\n",
      "18:41:03.976043  |  Epoch 25  |  Training loss 6.01938\n",
      "18:42:20.180093  |  Epoch 30  |  Training loss 5.91513\n",
      "18:43:38.349696  |  Epoch 35  |  Training loss 5.83905\n",
      "18:44:56.250277  |  Epoch 40  |  Training loss 5.77988\n",
      "18:46:14.018901  |  Epoch 45  |  Training loss 5.73257\n",
      "18:47:32.828554  |  Epoch 50  |  Training loss 5.69412\n",
      "accuracy:        0.07359\n",
      "training time:   771 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr:             0.001\n",
      "hidden size:     24\n",
      "18:47:53.965920  |  Epoch 1  |  Training loss 6.95448\n",
      "18:49:17.662856  |  Epoch 5  |  Training loss 6.77736\n",
      "18:51:01.586662  |  Epoch 10  |  Training loss 6.28719\n",
      "18:52:44.528091  |  Epoch 15  |  Training loss 5.99523\n",
      "18:54:28.190699  |  Epoch 20  |  Training loss 5.83195\n",
      "18:56:11.478774  |  Epoch 25  |  Training loss 5.73311\n",
      "18:57:56.126090  |  Epoch 30  |  Training loss 5.67018\n",
      "18:59:39.778529  |  Epoch 35  |  Training loss 5.62688\n",
      "19:01:23.334683  |  Epoch 40  |  Training loss 5.59506\n",
      "19:03:06.555794  |  Epoch 45  |  Training loss 5.57038\n",
      "19:04:50.076694  |  Epoch 50  |  Training loss 5.55021\n",
      "accuracy:        0.07359\n",
      "training time:   1037 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr:             0.005\n",
      "hidden size:     12\n",
      "19:05:06.069482  |  Epoch 1  |  Training loss 6.89123\n",
      "19:06:08.338019  |  Epoch 5  |  Training loss 6.01097\n",
      "19:07:26.417516  |  Epoch 10  |  Training loss 5.67831\n",
      "19:08:45.575638  |  Epoch 15  |  Training loss 5.56956\n",
      "19:10:05.422363  |  Epoch 20  |  Training loss 5.51583\n",
      "19:11:24.654418  |  Epoch 25  |  Training loss 5.48195\n",
      "19:12:43.312100  |  Epoch 30  |  Training loss 5.45010\n",
      "19:14:01.583951  |  Epoch 35  |  Training loss 5.42882\n",
      "19:15:21.648898  |  Epoch 40  |  Training loss 5.41119\n",
      "19:16:42.845134  |  Epoch 45  |  Training loss 5.39385\n",
      "19:18:03.299599  |  Epoch 50  |  Training loss 5.37605\n",
      "accuracy:        0.07700\n",
      "training time:   793 seconds\n",
      "\n",
      " =========================================================\n",
      "  Current parameters:\n",
      "lr:             0.005\n",
      "hidden size:     24\n",
      "19:18:24.235335  |  Epoch 1  |  Training loss 6.87873\n",
      "19:19:46.695034  |  Epoch 5  |  Training loss 5.80260\n",
      "19:21:32.716338  |  Epoch 10  |  Training loss 5.55560\n",
      "19:23:15.419270  |  Epoch 15  |  Training loss 5.47319\n",
      "19:25:00.440047  |  Epoch 20  |  Training loss 5.41798\n",
      "19:26:44.322400  |  Epoch 25  |  Training loss 5.37308\n",
      "19:28:29.097783  |  Epoch 30  |  Training loss 5.33955\n",
      "19:30:13.328590  |  Epoch 35  |  Training loss 5.31295\n",
      "19:31:57.179668  |  Epoch 40  |  Training loss 5.28950\n",
      "19:33:39.945225  |  Epoch 45  |  Training loss 5.26852\n",
      "19:35:23.231265  |  Epoch 50  |  Training loss 5.24884\n",
      "accuracy:        0.10974\n",
      "training time:   1039 seconds\n"
     ]
    }
   ],
   "source": [
    "wordembedding = embeddingmodel.embedding\n",
    "\n",
    "# hyperparameters\n",
    "lr_list = [0.001, 0.005]\n",
    "hidden_size_list = [12, 24]\n",
    "\n",
    "n_epochs = 50\n",
    "loss_fn = nn.CrossEntropyLoss() # Cross-entropy loss\n",
    "\n",
    "print(f\"Training on device {device}.\")\n",
    "best_model_3 = next_model_selection(lr_list, hidden_size_list, data_train_3, data_val_3, wordembedding,\n",
    "                                                                                        vocab_size, n_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0fd88",
   "metadata": {},
   "source": [
    "##### Save and load best model hyperparameters\n",
    "We save the dictionary containing which model is best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86fa35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'bestmodel_dict_next.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model_3, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae658e",
   "metadata": {},
   "source": [
    "And to load the dictionary back from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d58f9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'bestmodel_dict_next.pkl', 'rb') as file_to_read:\n",
    "    best_model_3 = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b3d11f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.005,\n",
       " 'hidden size': 24,\n",
       " 'accuracy': tensor(0.1097),\n",
       " 'number': 3,\n",
       " 'seconds': 1039}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05144d",
   "metadata": {},
   "source": [
    "##### Load trained model\n",
    "We then load the best models state dictionary to the myRNN architecture to get back the trained (best) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92914ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordembedding = embeddingmodel.embedding\n",
    "\n",
    "# architecture\n",
    "nextmodel = myRNN(wordembedding, vocab_size, best_model_3['hidden size'])\n",
    "\n",
    "# fill with trained parameters\n",
    "nextmodel.load_state_dict(torch.load(path + 'model_weights_next_' + str(best_model_3['number']) + '.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a9203",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "We can then calculate training accuracy and evaluate (test accuracy) the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7fcf938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:      0.10716\n",
      "Validation accuracy:    0.10974\n",
      "Test accuracy:          0.12495\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy_3(dataset):\n",
    "    \"\"\"\n",
    "    take context/target datasets and calculate accuracy\n",
    "    \"\"\"\n",
    "    target = dataset[:][1]\n",
    "    pred = nextmodel(dataset[:][0])\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    \n",
    "    return accuracy(pred, target.flatten())\n",
    "\n",
    "acc_train = get_accuracy_3(data_train_3)\n",
    "\n",
    "# evaluate model (test accuracy)\n",
    "acc_test = get_accuracy_3(data_test_3)\n",
    "            \n",
    "print('Training accuracy:      {:.5f}'.format(acc_train))\n",
    "print('Validation accuracy:    {:.5f}'.format(best_model_3['accuracy']))\n",
    "print('Test accuracy:          {:.5f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf96331",
   "metadata": {},
   "source": [
    "### 2.3.3 Beam search\n",
    "\n",
    "The beam search algorithm is implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1ac0801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(word_list, # sentence in list format\n",
    "                model,     # the model used to predict next word\n",
    "                n=5,       # number of additional words to predict\n",
    "                B=3):      # branch factor\n",
    "    \n",
    "    inputs = torch.tensor(vocab.lookup_indices(text_input))\n",
    "    sequences = [[inputs.tolist(), 1.0]]\n",
    "    \n",
    "    for _ in range(n):\n",
    "        all_candidates = []\n",
    "        for i in range(len(sequences)):\n",
    "            \n",
    "            seq, score = sequences[i]\n",
    "            pred = model(torch.tensor(seq))\n",
    "            \n",
    "            for j in range(vocab_size):\n",
    "                candidate = [seq + [j], score*pred[j].tolist()]\n",
    "                all_candidates.append(candidate)\n",
    "                \n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1], reverse=True)\n",
    "        sequences = ordered[:B]\n",
    "            \n",
    "    return vocab.lookup_tokens(sequences[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00377af",
   "metadata": {},
   "source": [
    "### 2.3.4 Playing with the model\n",
    "\n",
    "Now we will test our word predictor. We use sequences of arbitrary length and give it to the model. We let the model complete the next n words using the beam search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6c585144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he was a was of the was of'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['he', 'was', 'a']\n",
    "words = beam_search(text_input, nextmodel, n=5, B=3)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d6f88965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there are two minutes to the he of the to of'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['there', 'are', 'two', 'minutes', 'to', 'the']\n",
    "words = beam_search(text_input, nextmodel, n=5, B=3)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3fd988fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i wish i had of the was of the'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['i', 'wish', 'i', 'had']\n",
    "words = beam_search(text_input, nextmodel, n=5, B=3)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1ae2b6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you never were my to the the and of'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['you', 'never', 'were', 'my']\n",
    "words = beam_search(text_input, nextmodel, n=5, B=4)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1845493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the gentleman read my of the'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['the', 'gentleman', 'read', 'my']\n",
    "words = beam_search(text_input, nextmodel, n=2, B=3)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "22dd040a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you and i could never do of the to of'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input = ['you', 'and', 'i', 'could', 'never', 'do']\n",
    "words = beam_search(text_input, nextmodel, n=4, B=2)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f45adf",
   "metadata": {},
   "source": [
    "See pdf report for comments on these results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
